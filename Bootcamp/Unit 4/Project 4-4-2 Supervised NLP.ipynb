{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Supervised Natural Language Processing (NLP) invovles traing a model to label text.  The approach below involves cleaning the text, identifying the most commonly used words in each of the two texts in question and using the set of those two lists as features for the model.  Each text snippet is then tokenized and processed into a vector of 0s and 1s based on whether the snippet contains the feature-words.  Finally various classifiers are trained and tested.  \n",
    "\n",
    "This is the simplest case however. Could we get better performance by expanding our feature set?\n",
    "\n",
    "Additionally, one of the issues implicit in training a model like this is the extent to which it generalizes.  In the below investigation, I will start by training a model to discern between Lewis Carroll's _Alice in Wonderland_ and Jane Austen's _Persuasion_.  However, it begs the question(s), does _Persuasion_ generalize to Austen in general? Or _Alice_ to Lewis Carroll? Would a model trained on _Alice_ and _Persuasion_ still be able to classify Austen and Carroll if the Austen sample was from _Emma_? Let's probe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "# Load and clean the data.\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "persuasion_base = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice_base = re.sub(r'CHAPTER .*', '', alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alice = text_cleaner(alice_base)\n",
    "persuasion = text_cleaner(persuasion_base)\n",
    "# question: the text cleaner appears to clean punctuation but the below text still includes it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
       "3                                      (Oh, dear, !)  Carroll\n",
       "4                         (I, shall, be, late, !, ')  Carroll"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "I have adapted the code introduced in the project base text so that there is preallocation of dataframe space and there is optional inclusion of sentence stats (discussed below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "\n",
    "# POS dict\n",
    "pos_d = {'VERB':'verb_ct', 'NOUN':'noun_ct', 'ADV':'adv_ct', 'ADP':'adp_ct', \n",
    "         'PROPN':'propn_ct', 'ADJ':'adj_ct', 'DET':'det_ct', 'PUNCT':'punct_ct'}\n",
    "\n",
    "def bow_features(quotes, common_words, **kwargs):\n",
    "    print(len(quotes))\n",
    "    \n",
    "    # sentence stats\n",
    "    sent_stats = ['comma_ct', 'word_ct', 'adv_ct', 'adp_ct', 'propn_ct', 'adj_ct', 'punct_ct'] #'verb_ct', 'noun_ct','det_ct',\n",
    "    if 'sent_stats' in kwargs:\n",
    "        df = pd.DataFrame(columns=list(common_words) + sent_stats)\n",
    "        cols = list(common_words)+sent_stats\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=common_words)\n",
    "        cols = list(common_words)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        df[col] = np.zeros(len(quotes[0]))\n",
    "    df['text_sentence'] = quotes[0] \n",
    "    df['text_source'] = quotes[1]\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, quote in enumerate(df['text_sentence']):\n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in quote\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            try:\n",
    "                df.loc[i, word] += 1\n",
    "            except:\n",
    "                print(word)\n",
    "        \n",
    "        # add sentence features\n",
    "        if 'sent_stats' in kwargs:\n",
    "            commas = 0\n",
    "            for token in quote:\n",
    "                if token.orth_ == ',':\n",
    "                    commas += 1\n",
    "            df.loc[i, 'comma_ct'] = commas\n",
    "                    \n",
    "            c = Counter([token.pos_ for token in quote])\n",
    "            for key in pos_d.keys():\n",
    "                if key in c.keys():\n",
    "                    df.loc[i, pos_d[key]] = c[key]\n",
    "                else:\n",
    "                    df.loc[i, pos_d[key]] = c[key]\n",
    "            \n",
    "            df.loc[i, 'word_ct'] = len([token for token in quote if (not token.is_punct)])\n",
    "\n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the bags.\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(alicewords + persuasionwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point there has only been one text prep pipeline. Now it will fork into two. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 0:  \n",
    "### Add features to improve model performance  \n",
    "\n",
    "`word_counts` is composed entirely of words. However for `word_counts_ss` I added features that pertain to the entire sentence (rather than the culled token list) such as counts for various parts of speech, comma count, and word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5318\n",
      "made it to the loop\n",
      "Processing row 0\n",
      "Processing row 1000\n",
      "Processing row 2000\n",
      "Processing row 3000\n",
      "Processing row 4000\n",
      "Processing row 5000\n"
     ]
    }
   ],
   "source": [
    "word_counts = bow_features(sentences, common_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5318\n",
      "made it to the loop\n",
      "Processing row 0\n",
      "Processing row 1000\n",
      "Processing row 2000\n",
      "Processing row 3000\n",
      "Processing row 4000\n",
      "Processing row 5000\n"
     ]
    }
   ],
   "source": [
    "# this dataframe includes additional grammatical and symantic details as features \n",
    "word_counts_ss = bow_features(sentences, common_words, sent_stats= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting out train/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test(word_count_df):\n",
    "    Y = word_count_df['text_source']\n",
    "    X = word_count_df.iloc[:, ~word_count_df.columns.isin(['text_sentence','text_source'])]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                        Y,\n",
    "                                                        test_size=0.3,\n",
    "                                                        random_state=0)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ss, X_test_ss, y_train_ss, y_test_ss = make_train_test(word_counts_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = make_train_test(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def log_reg(_X_train, _X_test, _y_train, _y_test, **kwargs):\n",
    "    if 'params' in kwargs:\n",
    "        params = kwargs['params']\n",
    "        _lr = LogisticRegression(**params)\n",
    "    else:\n",
    "        _lr = LogisticRegression(solver = 'lbfgs')\n",
    "        \n",
    "    _lr.fit(_X_train, _y_train)\n",
    "    print(_X_train.shape, _y_train.shape)\n",
    "    print('Training set score:', _lr.score(_X_train, _y_train))\n",
    "    print('\\nTest set score:', _lr.score(_X_test, _y_test))\n",
    "    _y_pred = _lr.predict(_X_test)\n",
    "    print(pd.crosstab(_y_test, _y_pred))\n",
    "    return _lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3722, 3062) (3722,)\n",
      "Training set score: 0.9575497044599678\n",
      "\n",
      "Test set score: 0.9179197994987469\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1080       27\n",
      "Carroll         104      385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.92109501, 0.88888889, 0.91304348, 0.93558776, 0.90145396,\n",
       "       0.90468498])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#As a reference, I re-ran the model with only word features.\n",
    "lr = log_reg(X_train, X_test, y_train, y_test)\n",
    "\n",
    "cross_val_score(lr, X_train, y_train, cv = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 12 candidates, totalling 72 fits\n",
      "[CV] C=1, penalty=l2, solver=newton-cg ...............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1, penalty=l2, solver=newton-cg, score=0.9210950080515298, total=   1.4s\n",
      "[CV] C=1, penalty=l2, solver=newton-cg ...............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1, penalty=l2, solver=newton-cg, score=0.8888888888888888, total=   1.3s\n",
      "[CV] C=1, penalty=l2, solver=newton-cg ...............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    2.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1, penalty=l2, solver=newton-cg, score=0.9130434782608695, total=   1.2s\n",
      "[CV] C=1, penalty=l2, solver=newton-cg ...............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    4.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1, penalty=l2, solver=newton-cg, score=0.9355877616747182, total=   1.4s\n",
      "[CV] C=1, penalty=l2, solver=newton-cg ...............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    5.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1, penalty=l2, solver=newton-cg, score=0.901453957996769, total=   1.3s\n",
      "[CV] C=1, penalty=l2, solver=newton-cg ...............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    6.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1, penalty=l2, solver=newton-cg, score=0.9046849757673667, total=   1.3s\n",
      "[CV] C=1, penalty=l2, solver=lbfgs ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    8.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1, penalty=l2, solver=lbfgs, score=0.9210950080515298, total=   0.5s\n",
      "[CV] C=1, penalty=l2, solver=lbfgs ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    8.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1, penalty=l2, solver=lbfgs, score=0.8888888888888888, total=   0.7s\n",
      "[CV] C=1, penalty=l2, solver=lbfgs ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    9.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1, penalty=l2, solver=lbfgs, score=0.9130434782608695, total=   0.7s\n",
      "[CV] C=1, penalty=l2, solver=lbfgs ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:   10.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1, penalty=l2, solver=lbfgs, score=0.9355877616747182, total=   0.6s\n",
      "[CV] C=1, penalty=l2, solver=lbfgs ...................................\n",
      "[CV]  C=1, penalty=l2, solver=lbfgs, score=0.901453957996769, total=   0.6s\n",
      "[CV] C=1, penalty=l2, solver=lbfgs ...................................\n",
      "[CV]  C=1, penalty=l2, solver=lbfgs, score=0.9046849757673667, total=   0.5s\n",
      "[CV] C=1, penalty=l2, solver=liblinear ...............................\n",
      "[CV]  C=1, penalty=l2, solver=liblinear, score=0.9210950080515298, total=   0.2s\n",
      "[CV] C=1, penalty=l2, solver=liblinear ...............................\n",
      "[CV]  C=1, penalty=l2, solver=liblinear, score=0.8888888888888888, total=   0.2s\n",
      "[CV] C=1, penalty=l2, solver=liblinear ...............................\n",
      "[CV]  C=1, penalty=l2, solver=liblinear, score=0.9130434782608695, total=   0.2s\n",
      "[CV] C=1, penalty=l2, solver=liblinear ...............................\n",
      "[CV]  C=1, penalty=l2, solver=liblinear, score=0.9355877616747182, total=   0.2s\n",
      "[CV] C=1, penalty=l2, solver=liblinear ...............................\n",
      "[CV]  C=1, penalty=l2, solver=liblinear, score=0.901453957996769, total=   0.2s\n",
      "[CV] C=1, penalty=l2, solver=liblinear ...............................\n",
      "[CV]  C=1, penalty=l2, solver=liblinear, score=0.9046849757673667, total=   0.2s\n",
      "[CV] C=1, penalty=l2, solver=sag .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1, penalty=l2, solver=sag, score=0.9210950080515298, total=   9.3s\n",
      "[CV] C=1, penalty=l2, solver=sag .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1, penalty=l2, solver=sag, score=0.8888888888888888, total=   8.3s\n",
      "[CV] C=1, penalty=l2, solver=sag .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1, penalty=l2, solver=sag, score=0.9130434782608695, total=   8.3s\n",
      "[CV] C=1, penalty=l2, solver=sag .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1, penalty=l2, solver=sag, score=0.9355877616747182, total=   8.8s\n",
      "[CV] C=1, penalty=l2, solver=sag .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1, penalty=l2, solver=sag, score=0.901453957996769, total=  11.1s\n",
      "[CV] C=1, penalty=l2, solver=sag .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1, penalty=l2, solver=sag, score=0.9046849757673667, total=   9.0s\n",
      "[CV] C=10, penalty=l2, solver=newton-cg ..............................\n",
      "[CV]  C=10, penalty=l2, solver=newton-cg, score=0.9082125603864735, total=   2.0s\n",
      "[CV] C=10, penalty=l2, solver=newton-cg ..............................\n",
      "[CV]  C=10, penalty=l2, solver=newton-cg, score=0.8969404186795491, total=   1.7s\n",
      "[CV] C=10, penalty=l2, solver=newton-cg ..............................\n",
      "[CV]  C=10, penalty=l2, solver=newton-cg, score=0.9114331723027376, total=   1.8s\n",
      "[CV] C=10, penalty=l2, solver=newton-cg ..............................\n",
      "[CV]  C=10, penalty=l2, solver=newton-cg, score=0.9388083735909822, total=   2.0s\n",
      "[CV] C=10, penalty=l2, solver=newton-cg ..............................\n",
      "[CV]  C=10, penalty=l2, solver=newton-cg, score=0.901453957996769, total=   1.6s\n",
      "[CV] C=10, penalty=l2, solver=newton-cg ..............................\n",
      "[CV]  C=10, penalty=l2, solver=newton-cg, score=0.9063004846526656, total=   1.7s\n",
      "[CV] C=10, penalty=l2, solver=lbfgs ..................................\n",
      "[CV]  C=10, penalty=l2, solver=lbfgs, score=0.9082125603864735, total=   1.0s\n",
      "[CV] C=10, penalty=l2, solver=lbfgs ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, penalty=l2, solver=lbfgs, score=0.8969404186795491, total=   0.9s\n",
      "[CV] C=10, penalty=l2, solver=lbfgs ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, penalty=l2, solver=lbfgs, score=0.9114331723027376, total=   0.9s\n",
      "[CV] C=10, penalty=l2, solver=lbfgs ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, penalty=l2, solver=lbfgs, score=0.9388083735909822, total=   1.0s\n",
      "[CV] C=10, penalty=l2, solver=lbfgs ..................................\n",
      "[CV]  C=10, penalty=l2, solver=lbfgs, score=0.901453957996769, total=   1.1s\n",
      "[CV] C=10, penalty=l2, solver=lbfgs ..................................\n",
      "[CV]  C=10, penalty=l2, solver=lbfgs, score=0.9063004846526656, total=   1.0s\n",
      "[CV] C=10, penalty=l2, solver=liblinear ..............................\n",
      "[CV]  C=10, penalty=l2, solver=liblinear, score=0.9082125603864735, total=   0.2s\n",
      "[CV] C=10, penalty=l2, solver=liblinear ..............................\n",
      "[CV]  C=10, penalty=l2, solver=liblinear, score=0.8969404186795491, total=   0.2s\n",
      "[CV] C=10, penalty=l2, solver=liblinear ..............................\n",
      "[CV]  C=10, penalty=l2, solver=liblinear, score=0.9114331723027376, total=   0.2s\n",
      "[CV] C=10, penalty=l2, solver=liblinear ..............................\n",
      "[CV]  C=10, penalty=l2, solver=liblinear, score=0.9388083735909822, total=   0.1s\n",
      "[CV] C=10, penalty=l2, solver=liblinear ..............................\n",
      "[CV]  C=10, penalty=l2, solver=liblinear, score=0.901453957996769, total=   0.2s\n",
      "[CV] C=10, penalty=l2, solver=liblinear ..............................\n",
      "[CV]  C=10, penalty=l2, solver=liblinear, score=0.9063004846526656, total=   0.2s\n",
      "[CV] C=10, penalty=l2, solver=sag ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, penalty=l2, solver=sag, score=0.9130434782608695, total=  10.0s\n",
      "[CV] C=10, penalty=l2, solver=sag ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, penalty=l2, solver=sag, score=0.9033816425120773, total=   8.3s\n",
      "[CV] C=10, penalty=l2, solver=sag ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, penalty=l2, solver=sag, score=0.9130434782608695, total=   8.9s\n",
      "[CV] C=10, penalty=l2, solver=sag ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, penalty=l2, solver=sag, score=0.9452495974235104, total=   8.6s\n",
      "[CV] C=10, penalty=l2, solver=sag ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, penalty=l2, solver=sag, score=0.9046849757673667, total=   9.1s\n",
      "[CV] C=10, penalty=l2, solver=sag ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, penalty=l2, solver=sag, score=0.901453957996769, total=   9.1s\n",
      "[CV] C=100, penalty=l2, solver=newton-cg .............................\n",
      "[CV]  C=100, penalty=l2, solver=newton-cg, score=0.9001610305958132, total=   2.4s\n",
      "[CV] C=100, penalty=l2, solver=newton-cg .............................\n",
      "[CV]  C=100, penalty=l2, solver=newton-cg, score=0.8727858293075684, total=   2.2s\n",
      "[CV] C=100, penalty=l2, solver=newton-cg .............................\n",
      "[CV]  C=100, penalty=l2, solver=newton-cg, score=0.895330112721417, total=   2.3s\n",
      "[CV] C=100, penalty=l2, solver=newton-cg .............................\n",
      "[CV]  C=100, penalty=l2, solver=newton-cg, score=0.9259259259259259, total=   2.3s\n",
      "[CV] C=100, penalty=l2, solver=newton-cg .............................\n",
      "[CV]  C=100, penalty=l2, solver=newton-cg, score=0.8723747980613893, total=   2.0s\n",
      "[CV] C=100, penalty=l2, solver=newton-cg .............................\n",
      "[CV]  C=100, penalty=l2, solver=newton-cg, score=0.8966074313408724, total=   2.3s\n",
      "[CV] C=100, penalty=l2, solver=lbfgs .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=100, penalty=l2, solver=lbfgs, score=0.9001610305958132, total=   1.1s\n",
      "[CV] C=100, penalty=l2, solver=lbfgs .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=100, penalty=l2, solver=lbfgs, score=0.8743961352657005, total=   0.9s\n",
      "[CV] C=100, penalty=l2, solver=lbfgs .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=100, penalty=l2, solver=lbfgs, score=0.893719806763285, total=   1.0s\n",
      "[CV] C=100, penalty=l2, solver=lbfgs .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=100, penalty=l2, solver=lbfgs, score=0.927536231884058, total=   1.0s\n",
      "[CV] C=100, penalty=l2, solver=lbfgs .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=100, penalty=l2, solver=lbfgs, score=0.8723747980613893, total=   1.0s\n",
      "[CV] C=100, penalty=l2, solver=lbfgs .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=100, penalty=l2, solver=lbfgs, score=0.8966074313408724, total=   1.1s\n",
      "[CV] C=100, penalty=l2, solver=liblinear .............................\n",
      "[CV]  C=100, penalty=l2, solver=liblinear, score=0.9001610305958132, total=   0.2s\n",
      "[CV] C=100, penalty=l2, solver=liblinear .............................\n",
      "[CV]  C=100, penalty=l2, solver=liblinear, score=0.8727858293075684, total=   0.2s\n",
      "[CV] C=100, penalty=l2, solver=liblinear .............................\n",
      "[CV]  C=100, penalty=l2, solver=liblinear, score=0.895330112721417, total=   0.2s\n",
      "[CV] C=100, penalty=l2, solver=liblinear .............................\n",
      "[CV]  C=100, penalty=l2, solver=liblinear, score=0.9259259259259259, total=   0.2s\n",
      "[CV] C=100, penalty=l2, solver=liblinear .............................\n",
      "[CV]  C=100, penalty=l2, solver=liblinear, score=0.8723747980613893, total=   0.2s\n",
      "[CV] C=100, penalty=l2, solver=liblinear .............................\n",
      "[CV]  C=100, penalty=l2, solver=liblinear, score=0.8966074313408724, total=   0.2s\n",
      "[CV] C=100, penalty=l2, solver=sag ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=100, penalty=l2, solver=sag, score=0.9146537842190016, total=   8.3s\n",
      "[CV] C=100, penalty=l2, solver=sag ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=100, penalty=l2, solver=sag, score=0.9066022544283414, total=  10.4s\n",
      "[CV] C=100, penalty=l2, solver=sag ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=100, penalty=l2, solver=sag, score=0.9130434782608695, total=   8.3s\n",
      "[CV] C=100, penalty=l2, solver=sag ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=100, penalty=l2, solver=sag, score=0.9468599033816425, total=   9.2s\n",
      "[CV] C=100, penalty=l2, solver=sag ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=100, penalty=l2, solver=sag, score=0.8998384491114702, total=   8.7s\n",
      "[CV] C=100, penalty=l2, solver=sag ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=100, penalty=l2, solver=sag, score=0.8998384491114702, total=   9.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# I ran a parameter search to see if I could improve the model by configuration before adding features\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "            'penalty':['l2'],\n",
    "            'C':[1,10,100],\n",
    "            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag'],\n",
    "        }\n",
    "lr = LogisticRegression()\n",
    "GS = GridSearchCV(lr, parameters,cv=6,verbose=10)\n",
    "GS.fit(X_train,y_train)\n",
    "\n",
    "new_params = GS.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9134873723804406\n",
      "{'C': 10, 'penalty': 'l2', 'solver': 'sag'}\n"
     ]
    }
   ],
   "source": [
    "print(GS.best_score_)\n",
    "print(GS.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interestingly, on another run, this combination fared better, which suggests a small difference between configurations.\n",
    "params_d = {'C':10, 'penalty':'l2', 'solver':'liblinear'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3722, 3062) (3722,)\n",
      "Training set score: 0.9572810317033853\n",
      "\n",
      "Test set score: 0.918546365914787\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1081       26\n",
      "Carroll         104      385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# retrained model with updated parameters\n",
    "lr = log_reg(X_train, X_test, y_train, y_test, params = params_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3722, 3072) (3722,)\n",
      "Training set score: 0.9841483073616335\n",
      "\n",
      "Test set score: 0.918546365914787\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1065       42\n",
      "Carroll          88      401\n"
     ]
    }
   ],
   "source": [
    "# trained the model with the additional features\n",
    "lr_ss = log_reg(X_train_ss, X_test_ss, y_train_ss, y_test_ss, params = params_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1:\n",
    "Now I will look into those questions about model generalizability.  \n",
    "\n",
    "1. ) Is the model good at discerning _Alice_?  Which is to say, can the existing model discern between _Alice_ and something the model hasn't seen before (NOT _Alice_)?  \n",
    "2. ) Is the model good at discerning _Persuasion_? Similar to (1), can the existing model discern between _Persuasion_ and something the model hasn't seen before (NOT _Persuasion_)?  \n",
    "3. ) Is the model good at discerning Austen in general?  If the model is fed a mix of _Persuasion_ and _Emma_ and _Alice_, will it be able to correctly classify both _Persuasion_ and _Emma_ as Austen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few functions for making various test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make author set\n",
    "def make_author_test(_X_test, _y_test, target_author):\n",
    "    _X_test['target'] = _y_test\n",
    "    df_test_author = _X_test[_X_test.target == target_author]\n",
    "\n",
    "    X_test_author = df_test_author.iloc[:, ~df_test_author.columns.isin(['target'])] #X_test[y_test[y_test == 'Carroll'].index]\n",
    "    y_test_author = df_test_author['target']\n",
    "    return X_test_author, y_test_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make NOT author set\n",
    "def make_nonauthor_test(df_nonauthor_bow, nonauthor, sample_size):\n",
    "    df_nonauthor_bow['text_source'] = [nonauthor for ik in range(len(df_nonauthor_bow))]\n",
    "\n",
    "    subsample_nonauthor = df_nonauthor_bow.sample(n=sample_size)\n",
    "    X_test_nonauthor = subsample_nonauthor.iloc[:, ~subsample_nonauthor.columns.isin(['text_sentence','text_source']) ]\n",
    "    y_test_nonauthor = subsample_nonauthor['text_source']\n",
    "    return X_test_nonauthor, y_test_nonauthor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate test sets \n",
    "def make_full_sets(X_test_author, y_test_author, X_test_nonauthor, y_test_nonauthor):\n",
    "#     X_test_author, y_test_author = test_author[0], test_author[1]\n",
    "#     X_test_nonauthor, y_test_nonauthor = test_nonauthor[0], test_nonauthor[1]\n",
    "    X_test_AnonA = np.concatenate([X_test_author, X_test_nonauthor], axis=0)\n",
    "    y_test_AnonA = np.concatenate([y_test_author, y_test_nonauthor], axis=0)\n",
    "    return X_test_AnonA, y_test_AnonA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep _Paradise Lost_ by John Milton as an alternative text text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book I Of Man's first disobedience, and the fruit Of that forbidden tree whose mortal taste Brought \n"
     ]
    }
   ],
   "source": [
    "# another work:\n",
    "paradise_base = gutenberg.raw('milton-paradise.txt')\n",
    "paradise_base = re.sub(r'VOLUME \\w+', '', paradise_base)\n",
    "paradise_base = re.sub(r'CHAPTER \\w+', '', paradise_base)\n",
    "paradise = text_cleaner(paradise_base)\n",
    "print(paradise[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse our cleaned data.\n",
    "paradise_doc = nlp(paradise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group into sentences.\n",
    "paradise_sents = [[sent, \"Not\"] for sent in paradise_doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nnormalize to the same length or Alice\n",
    "paradise_sents = paradise_sents[0:len(alice_sents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1669\n",
      "made it to the loop\n",
      "Processing row 0\n",
      "Processing row 1000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Bag of words for Paradise Lost\n",
    "paradise_sentences = pd.DataFrame(paradise_sents)\n",
    "paradise_bow = bow_features(paradise_sentences, common_words)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1669\n",
      "made it to the loop\n",
      "Processing row 0\n",
      "Processing row 1000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Bage of words for Paradise Lost with sentence features\n",
    "paradise_bow_ss = bow_features(paradise_sentences, common_words, sent_stats = True)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alice v. any other work  \n",
    "\n",
    "In this case, the `Austen` label will be interpretted as **not** `Carroll`, so the test data will be prepped with `Austen` labels, added to the `Carroll` test set and scored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_Carroll, y_test_Carroll = make_author_test(X_test, y_test, 'Carroll')\n",
    "X_test_nonCarroll, y_test_nonCarroll = make_nonauthor_test(paradise_bow, 'Austen', len(y_test_Carroll))\n",
    "\n",
    "X_test_CnonC, y_test_CnonC = make_full_sets(X_test_Carroll, y_test_Carroll, X_test_nonCarroll, y_test_nonCarroll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.7975460122699386\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Carroll</th>\n",
       "      <th>Not</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Austen</th>\n",
       "      <td>94</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carroll</th>\n",
       "      <td>385</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    Carroll  Not\n",
       "row_0                \n",
       "Austen        94  395\n",
       "Carroll      385  104"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run existing model over new test set\n",
    "print('\\nTest set score:', lr.score(X_test_CnonC, y_test_CnonC))\n",
    "lr_CnC_predicted = lr.predict(X_test_CnonC)\n",
    "preds = np.where(lr_CnC_predicted=='Austen', 'Not', lr_CnC_predicted) \n",
    "# y_test_relabeled = np.where(y_test_CnonC=='Austen', 'Not', y_test_CnonC) \n",
    "pd.crosstab(y_test_CnonC, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare to version with gramatical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_Carroll_ss, y_test_Carroll_ss = make_author_test(X_test_ss,y_test_ss, 'Carroll')\n",
    "X_test_nonCarroll_ss, y_test_nonCarroll_ss = make_nonauthor_test(paradise_bow_ss, 'Austen', len(y_test_Carroll))\n",
    "\n",
    "X_test_CnonC_ss, y_test_CnonC_ss = make_full_sets(X_test_Carroll_ss, y_test_Carroll_ss, X_test_nonCarroll_ss, y_test_nonCarroll_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.843558282208589\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Austen</th>\n",
       "      <th>Carroll</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Carroll</th>\n",
       "      <td>88</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Not</th>\n",
       "      <td>424</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    Austen  Carroll\n",
       "row_0                   \n",
       "Carroll      88      401\n",
       "Not         424       65"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run existing model over new test set\n",
    "print('\\nTest set score:', lr_ss.score(X_test_CnonC_ss, y_test_CnonC_ss))\n",
    "lr_CnC_predicted_ss = lr_ss.predict(X_test_CnonC_ss)\n",
    "# preds_ss = np.where(lr_CnC_predicted_ss=='Austen', 'Not', lr_CnC_predicted_ss) \n",
    "y_test_relabeled = np.where(y_test_CnonC=='Austen', 'Not', y_test_CnonC) \n",
    "pd.crosstab(y_test_relabeled, lr_CnC_predicted_ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results suggest that _Alice_ seperates well from _Paradise Lost_. If this were not the case, one would see  more \"Not\" examples classified as \"Carroll\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persuasion v. any other work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out the Persuasion test rows\n",
    "X_test_pers, y_test_pers = make_author_test(X_test, y_test, 'Austen')\n",
    "\n",
    "# prep the non Persuasion (Carroll) test rows\n",
    "X_test_nonPers, y_test_nonPers = make_nonauthor_test(paradise_bow, 'Carroll', len(y_test_pers))\n",
    "\n",
    "# concatenate into one set\n",
    "X_test_PnonP, y_test_PnonP = make_full_sets(X_test_pers, y_test_pers, X_test_nonPers, y_test_nonPers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.5844625112917796\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Austen</th>\n",
       "      <th>Carroll</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Austen</th>\n",
       "      <td>1081</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carroll</th>\n",
       "      <td>894</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    Austen  Carroll\n",
       "row_0                   \n",
       "Austen     1081       26\n",
       "Carroll     894      213"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run existing model over new test set\n",
    "print('\\nTest set score:', lr.score(X_test_PnonP, y_test_PnonP))\n",
    "lr_PnP_predicted = lr.predict(X_test_PnonP)\n",
    "pd.crosstab(y_test_PnonP, lr_PnP_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare to version with grammatical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out the Persuasion test rows\n",
    "X_test_pers_ss, y_test_pers_ss = make_author_test(X_test_ss, y_test_ss, 'Austen')\n",
    "\n",
    "# prep the non Persuasion (Carroll) test rows\n",
    "X_test_nonPers_ss, y_test_nonPers_ss = make_nonauthor_test(paradise_bow_ss, 'Carroll', len(y_test_pers_ss))\n",
    "\n",
    "# concatenate into one set\n",
    "X_test_PnonP_ss, y_test_PnonP_ss = make_full_sets(X_test_pers_ss, y_test_pers_ss, X_test_nonPers_ss, y_test_nonPers_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.53613369467028\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Austen</th>\n",
       "      <th>Carroll</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Austen</th>\n",
       "      <td>1065</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Not</th>\n",
       "      <td>985</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0   Austen  Carroll\n",
       "row_0                  \n",
       "Austen    1065       42\n",
       "Not        985      122"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run existing model over new test set\n",
    "print('\\nTest set score:', lr_ss.score(X_test_PnonP_ss, y_test_PnonP_ss))\n",
    "lr_PnP_predicted_ss = lr_ss.predict(X_test_PnonP_ss)\n",
    "y_test_relabeled_ss = np.where(y_test_PnonP_ss=='Carroll', 'Not', y_test_PnonP_ss) \n",
    "\n",
    "\n",
    "pd.crosstab(y_test_relabeled_ss, lr_PnP_predicted_ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, it seems that the model thinks everything is _Persuasion_.  Adding the grammatical features back in did not improve performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Austen v. any other work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to\n"
     ]
    }
   ],
   "source": [
    "# another work:\n",
    "emma_base = gutenberg.raw('austen-emma.txt')\n",
    "emma_base = re.sub(r'VOLUME \\w+', '', emma_base)\n",
    "emma_base = re.sub(r'CHAPTER \\w+', '', emma_base)\n",
    "emma = text_cleaner(emma_base)\n",
    "print(emma[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse our cleaned data.\n",
    "emma_doc = nlp(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group into sentences.\n",
    "emma_sents = [[sent, \"Austen\"] for sent in emma_doc.sents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emma is quite long, let's cut it down to the same length as Alice.\n",
    "emma_sents = emma_sents[0:len(alice_sents)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1669\n",
      "made it to the loop\n",
      "Processing row 0\n",
      "Processing row 1000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Build a new Bag of Words data frame for Emma word counts.\n",
    "emma_sentences = pd.DataFrame(emma_sents)\n",
    "emma_bow = bow_features(emma_sentences, common_words)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build smaller subsamples of Austen works so that the test set will be half Austen, half not Austen (in this case, Milton)\n",
    "subsample_emma = emma_bow.sample(n=int(np.floor(len(y_test_nonPers)/2)))\n",
    "X_test_emma = subsample_emma.iloc[:, ~subsample_emma.columns.isin(['text_sentence','text_source']) ]\n",
    "y_test_emma = subsample_emma['text_source']\n",
    "\n",
    "test_Pers = X_test_pers\n",
    "test_Pers['target'] = y_test_pers\n",
    "subsample_Pers = test_Pers.sample(n=len(y_test_nonPers)-len(y_test_emma))\n",
    "X_test_Pers = subsample_Pers.iloc[:, ~subsample_Pers.columns.isin(['target'])] #X_test[y_test[y_test == 'Carroll'].index]\n",
    "y_test_Pers = subsample_Pers['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((489, 3062), (244, 3062), (245, 3062), (1596, 3063), (245, 3063), 863)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_Carroll.shape, X_test_emma.shape, X_test_Pers.shape, X_test.shape, subsample_Pers.shape, len(y_test_pers)-len(y_test_emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate test sets to full Carroll-nonCarroll test set\n",
    "X_test_AunonC = np.concatenate([X_test_nonPers, X_test_emma, X_test_Pers], axis=0)\n",
    "y_test_AunonC = np.concatenate([y_test_nonPers, y_test_emma, y_test_Pers], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.573170731707317\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Austen</th>\n",
       "      <th>Carroll</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Austen</th>\n",
       "      <td>1056</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carroll</th>\n",
       "      <td>894</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    Austen  Carroll\n",
       "row_0                   \n",
       "Austen     1056       51\n",
       "Carroll     894      213"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run existing model over new test set\n",
    "print('\\nTest set score:', lr.score(X_test_AunonC, y_test_AunonC))\n",
    "lr_AunonC_predicted = lr.predict(X_test_AunonC)\n",
    "pd.crosstab(y_test_AunonC, lr_AunonC_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Gramatical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1669\n",
      "made it to the loop\n",
      "Processing row 0\n",
      "Processing row 1000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "emma_bow_ss = bow_features(emma_sentences, common_words, sent_stats = True)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build smaller subsamples of Austen works so that the test set will be half Austen, half not Austen (in this case, Milton)\n",
    "subsample_emma_ss = emma_bow_ss.sample(n=int(np.floor(len(y_test_nonPers_ss)/2)))\n",
    "X_test_emma_ss = subsample_emma_ss.iloc[:, ~subsample_emma_ss.columns.isin(['text_sentence','text_source']) ]\n",
    "y_test_emma_ss = subsample_emma_ss['text_source']\n",
    "\n",
    "test_Pers_ss = X_test_pers_ss\n",
    "test_Pers_ss['target'] = y_test_pers_ss\n",
    "subsample_Pers_ss = test_Pers_ss.sample(n=len(y_test_nonPers_ss)-len(y_test_emma_ss))\n",
    "X_test_Pers_ss = subsample_Pers_ss.iloc[:, ~subsample_Pers_ss.columns.isin(['target'])] #X_test[y_test[y_test == 'Carroll'].index]\n",
    "y_test_Pers_ss = subsample_Pers_ss['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate test sets to full Austen-nonAusten test set\n",
    "X_test_AunonC_ss = np.concatenate([X_test_nonPers_ss, X_test_emma_ss, X_test_Pers_ss], axis=0)\n",
    "y_test_AunonC_ss = np.concatenate([y_test_nonPers_ss, y_test_emma_ss, y_test_Pers_ss], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.5221318879855466\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Austen</th>\n",
       "      <th>Carroll</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Austen</th>\n",
       "      <td>1034</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carroll</th>\n",
       "      <td>985</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    Austen  Carroll\n",
       "row_0                   \n",
       "Austen     1034       73\n",
       "Carroll     985      122"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run existing model over new test set\n",
    "print('\\nTest set score:', lr_ss.score(X_test_AunonC_ss, y_test_AunonC_ss))\n",
    "lr_AunonC_predicted_ss = lr_ss.predict(X_test_AunonC_ss)\n",
    "pd.crosstab(y_test_AunonC_ss, lr_AunonC_predicted_ss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models did a fair job labelling Austen works correctly, but Carroll's _Alice_ did not turn out to be a catch-all example of another work.  The penalty on accuracy was in Milton sentences being incorrectly clasified as Austen lines rather than being labelled as \"Carroll\" (or \"NOT Austen\", as the case may be).  "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "49px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
