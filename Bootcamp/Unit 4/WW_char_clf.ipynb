{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_script(script):\n",
    "    script = script.lstrip('<pre>').split('\\nTHE END')[0]\n",
    "    script = script.replace('McGARRY', 'MCGARRY')\n",
    "    script = script.replace('CUT TO:\\n\\n', 'CUT TO: ')\n",
    "    script_ = re.sub(r'([A-Z])(\\nCUT TO:)', r'\\1\\n\\2',  script)\n",
    "    script = script_.replace('FADE OUT.\\nEND', '')\n",
    "    script = script_.replace('FADE OUT.\\n\\nEND ', '')\n",
    "    script = script.rstrip('\\n')\n",
    "     \n",
    "    return script#' '.join([act.split('\\n\\n',1)[1] for act in script.split('ACT ')[1:]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_script_df(script, episode_num):\n",
    "    bits = script.split('\\n\\n')\n",
    "    return pd.DataFrame({'elements':bits, 'episode': [episode_num for ik in range(len(bits))]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_stage_dir(script_df):\n",
    "    stage_dir = []\n",
    "    for ik, bit in enumerate(script_df.elements):\n",
    "        if (bit.split('\\n')[0].isupper() == False) and ('[' not in bit.split('\\n')[0]):\n",
    "            stage_dir.append(1)\n",
    "            script_df.loc[ik, 'elements'] = bit.replace('\\n', ' ')\n",
    "        else:\n",
    "            stage_dir.append(0)\n",
    "            \n",
    "    script_df['stage_dir'] = stage_dir\n",
    "    return script_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_scene_set(script_df):\n",
    "    scene_set = []\n",
    "    for elem in script_df.elements:\n",
    "        lines = elem.split('\\n')\n",
    "        if ('-' in lines[0]) and (lines[0].isupper()):\n",
    "            scene_set.append(1)\n",
    "        else:\n",
    "            scene_set.append(0)\n",
    "\n",
    "    script_df['scene_set'] = scene_set\n",
    "    return script_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def parse_lines_characters(script_df):\n",
    "    line = []\n",
    "    character = []\n",
    "    deliv_dir = []\n",
    "    audio_dir = []\n",
    "\n",
    "    for ik in range(len(script_df.elements)):\n",
    "        if (script_df.iloc[ik]['scene_set'] == 0) and (script_df.iloc[ik]['stage_dir'] == 0):\n",
    "            tmp_elem  = script_df.iloc[ik]['elements'].split('\\n')\n",
    "            \n",
    "            if len(tmp_elem)>1:\n",
    "                # character name & audio delivery notes\n",
    "                tmp_audio_dir = tmp_elem[0].split(' [')\n",
    "                if len(tmp_audio_dir)>1:\n",
    "                    audio_dir.append(tmp_audio_dir[1].strip(']'))\n",
    "                    character.append(tmp_audio_dir[0])\n",
    "                else:\n",
    "                    if (tmp_audio_dir[0].isupper() == True):\n",
    "                        audio_dir.append(np.nan)\n",
    "                        character.append(tmp_audio_dir[0])\n",
    "                    else:\n",
    "                        character.append(' ')\n",
    "                        audio_dir.append(np.nan)\n",
    "\n",
    "                # line & acting delivery notes\n",
    "                tmp_deliv_dir = tmp_elem[1].split('] ')\n",
    "                if len(tmp_deliv_dir)>1:\n",
    "                    deliv_dir.append(tmp_deliv_dir[0].strip('['))\n",
    "                    line.append(tmp_deliv_dir[1])\n",
    "                else:\n",
    "                    line.append(tmp_deliv_dir[0])\n",
    "                    deliv_dir.append(np.nan)\n",
    "            else:\n",
    "                line.append(np.nan)\n",
    "                character.append('Drop')\n",
    "                deliv_dir.append(np.nan)\n",
    "                audio_dir.append(np.nan)\n",
    "\n",
    "        else:\n",
    "            line.append(np.nan)\n",
    "            character.append(' ')\n",
    "            deliv_dir.append(np.nan)\n",
    "            audio_dir.append(np.nan)\n",
    "\n",
    "    script_df['line'] = line\n",
    "    script_df['character'] = character\n",
    "    script_df['deliv_dir'] = deliv_dir\n",
    "    script_df['audio_dir'] = audio_dir\n",
    "    \n",
    "    return script_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load scraped scripts\n",
    "ww_df = pd.read_json('WW.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull apart scripts\n",
    "script_dfs = []\n",
    "for ik, text in enumerate(ww_df.text):\n",
    "    ww_script = text\n",
    "    script_dfs.append(parse_lines_characters(parse_scene_set(parse_stage_dir(make_script_df(clean_script(ww_script),ik+1)))))\n",
    "\n",
    "# concatenate scripts\n",
    "ww_dfs = pd.concat(script_dfs, axis=0, join='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with character names that aren't characters\n",
    "for char in ['DISSOLVE TO', 'FADE OUT.', 'SMASH CUT TO: MAIN TITLES.', 'THE WEST WING', 'ACT ONE', 'ACT TWO', 'END TEASER', 'Drop']:\n",
    "    ww_dfs = ww_dfs[~ww_dfs['character'].str.contains( char)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize names \n",
    "names_d = {'C.J.': 'C.J. CREGG', 'DONNA':'DONNA MOSS', 'LEO':'LEO MCGARRY', 'JOSH': 'JOSH LYMAN', 'BILLY': 'BILLY KENWORTHY', 'MARY': 'MARY MARSH', 'SAM': 'SAM SEABORN', 'TOBY': 'TOBY ZIEGLER', 'PRESIDENT JED BARTLET': 'BARTLET' }\n",
    "\n",
    "for key, value in names_d.items():\n",
    "    ww_dfs['character'] = ww_dfs['character'].str.replace(value, key)\n",
    "    ww_dfs['character'] = ww_dfs['character'].str.replace(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1664\n",
      "ABBEY 176\n",
      "ADVISOR 4\n",
      "AGENT 10\n",
      "AIDES 2\n",
      "AL 14\n",
      "AL KIEFER 1\n",
      "ALL 2\n",
      "ANNOUNCER 1\n",
      "ARMY GUY 1 3\n",
      "ARMY GUY 2 3\n",
      "ARMY GUY 3 1\n",
      "BAMBANG 12\n",
      "BARTLET 1690\n",
      "BILL 1\n",
      "BILLY KENWORTHY 27\n",
      "BOB 13\n",
      "BOBBI 5\n",
      "BOBBY 29\n",
      "BONNIE 31\n",
      "BRUCE 7\n",
      "BRUNO 17\n",
      "BURNS 12\n",
      "BUTTERFIELD 8\n",
      "C.J 2\n",
      "C.J. CREGG 1454\n",
      "CABINET OFFICERS 2\n",
      "CALDWELL 23\n",
      "CANDY 3\n",
      "CAPTAIN 5\n",
      "CARL 12\n",
      "CARMINE 1\n",
      "CAROL 52\n",
      "CATHY 62\n",
      "CHARLIE 348\n",
      "CHINESE AMBASSADOR 7\n",
      "CHRIS 11\n",
      "CIA DIRECTOR 5\n",
      "CLAYPOOL 42\n",
      "CONGRESSMAN 7\n",
      "CONGRESSMAN 1 6\n",
      "CONGRESSMAN 2 3\n",
      "CONGRESSWOMAN 1\n",
      "CROSSFIELD 1\n",
      "CROUCH 18\n",
      "CROWD 3\n",
      "CUT TO: 1\n",
      "CUT TO: INT. AIR FORCE ONE 1\n",
      "DAISY 40\n",
      "DANNY 392\n",
      "DAVID 1\n",
      "DAVID HASSELHOFF 1\n",
      "DIRECTOR 4\n",
      "DONNA MOSS 558\n",
      "DONNIE 1\n",
      "DRUMM 7\n",
      "ED 13\n",
      "EDGAR DRUMM 1\n",
      "EMMA 1\n",
      "EVERYONE 3\n",
      "FADE OUT 1\n",
      "FATHER CAVANAUGH 24\n",
      "FEMALE AIDE 4\n",
      "FITZWALLACE 83\n",
      "FLIGHT ATTENDANT 1 1\n",
      "FLIGHT ATTENDANT 2 5\n",
      "FLIGHT ATTENDANT 3 1\n",
      "FRED 3\n",
      "FRENCHMAN 1\n",
      "GENERAL 2\n",
      "GEORGE 16\n",
      "GINA 59\n",
      "GINA TOSCANO 1\n",
      "GINGER 17\n",
      "GIRL 3\n",
      "GIRLS 5\n",
      "GIRLS IN THE CROWD 1\n",
      "GLADMAN 16\n",
      "GOMEZ 6\n",
      "GRANT 3\n",
      "GUY 2\n",
      "GUY 1 19\n",
      "GUY 2 5\n",
      "GUY 3 3\n",
      "GUYS 1\n",
      "HACKETT 12\n",
      "HAROLD 12\n",
      "HAROLD LEWIS 1\n",
      "HARRISON 23\n",
      "HARRY 2\n",
      "HELEN 5\n",
      "HERALD 2\n",
      "HOMELESS MAN 15\n",
      "HOST 6\n",
      "HOYNES 139\n",
      "INDIAN AMBASSADOR 5\n",
      "INTELLIGENCE GUY 3\n",
      "JACK 3\n",
      "JANEANE 3\n",
      "JANET 2\n",
      "JAY 6\n",
      "JEFF 45\n",
      "JEFF BRECKENRIDGE 1\n",
      "JEFFREY 13\n",
      "JENNY 28\n",
      "JERRY 14\n",
      "JESSICA 2\n",
      "JOE 6\n",
      "JOEY 80\n",
      "JOHN VAN DYKE 1\n",
      "JOINT CHIEFS OF STAFF 1\n",
      "JONATHAN 1\n",
      "JOSH LYMAN 2188\n",
      "JUDGE 1\n",
      "KAREN 23\n",
      "KATIE 8\n",
      "KATZENMOYER 11\n",
      "KELLY 1\n",
      "KEN 10\n",
      "KENNY 4\n",
      "KIDS 10\n",
      "LACEY 7\n",
      "LARRY 15\n",
      "LAURIE 128\n",
      "LEELA 16\n",
      "LEO MCGARRY 1460\n",
      "LILLIENFIELD 4\n",
      "LILLY 27\n",
      "LITTLE 9\n",
      "LUTHER 2\n",
      "MAJOR 1\n",
      "MAJOR TATE 10\n",
      "MAJOR THOMPSON 10\n",
      "MALE AIDE 3\n",
      "MALLORY 218\n",
      "MAN 24\n",
      "MAN 1 10\n",
      "MAN 2 4\n",
      "MAN 3 1\n",
      "MANDY 549\n",
      "MARBURY 50\n",
      "MARCUS 41\n",
      "MARGARET 105\n",
      "MARK 13\n",
      "MARK MILLER 1\n",
      "MARY MARSH 19\n",
      "MENDOZA 25\n",
      "MIKE 21\n",
      "MILDRED 6\n",
      "MILITARY ADVISOR 1\n",
      "MILITARY GUY 1 3\n",
      "MILITARY GUY 2 4\n",
      "MINALDI 13\n",
      "MITCH 3\n",
      "MORRIS 57\n",
      "MR. LYDELL 4\n",
      "MRS LANDINGHAM 1\n",
      "MRS. LANDINGHAM 131\n",
      "MRS. LYDELL 7\n",
      "MRS.LANDINGHAM 1\n",
      "NANCY 14\n",
      "NANCY A 1\n",
      "NESSLER 14\n",
      "NEWSCASTER 1\n",
      "O'LEARY 22\n",
      "OFFICER 22\n",
      "OFFICER 1 3\n",
      "OFFICER 2 4\n",
      "OFFICER PETER 9\n",
      "P.D. 1 8\n",
      "PAKISTANI AMBASSADOR 12\n",
      "POLICE OFFICER 2\n",
      "PRESIDENT JED BARTLET 1\n",
      "RABBI 5\n",
      "RABBI GLASSMAN 16\n",
      "REESEMAN 30\n",
      "REPORTER 20\n",
      "REPORTER 1 6\n",
      "REPORTER 2 7\n",
      "REPORTER 3 9\n",
      "REPORTER 4 2\n",
      "REPORTERS 4\n",
      "RICHARDSON 9\n",
      "RINDELL 1\n",
      "ROGER 4\n",
      "ROLL CALL 2\n",
      "RON 21\n",
      "RON BUTTERFIELD 1\n",
      "RUSSELL 16\n",
      "RUSSO 9\n",
      "RUTH 1\n",
      "SAM SEABORN 1490\n",
      "SECRET SERVICE AGENT 1\n",
      "SENIOR STAFF 2\n",
      "SERGEANT MCNAMARA 1\n",
      "SIGUTO 7\n",
      "SIMON 18\n",
      "SINGER 1\n",
      "SKINNER 18\n",
      "SMASH CUT TO: MAIN TITLES 1\n",
      "SONDRA 5\n",
      "STACY 5\n",
      "STAFF 1\n",
      "STAFFER 9\n",
      "STAFFER 1 6\n",
      "STAFFER 2 4\n",
      "STAND WORKER 10\n",
      "STANLEY 24\n",
      "STEVE 23\n",
      "STEVEN 1\n",
      "STEVIE 5\n",
      "STUDENT 1\n",
      "STUDENT 1 8\n",
      "STUDENT 2 6\n",
      "STUDENTS 2\n",
      "SUBPOENA MAN 7\n",
      "SUZANNE 1\n",
      "T.V. 3\n",
      "TED MARCUS 1\n",
      "TILLINGHOUSE 13\n",
      "TOBY ZIEGLER 1286\n",
      "TOM 3\n",
      "TONY 1\n",
      "TRIBBY 8\n",
      "VAN DYKE 8\n",
      "VERONICA 4\n",
      "WAITER 3\n",
      "WHITE HOUSE GUARD 1\n",
      "WICK 22\n",
      "WILLIS 30\n",
      "WILSON 1\n",
      "WOMAN 36\n",
      "WOMAN IN T.V. 1\n",
      "WOMAN'S VOICE 1\n",
      "ZOEY 157\n"
     ]
    }
   ],
   "source": [
    "# check character lists\n",
    "character_grps = ww_dfs.groupby('character')\n",
    "\n",
    "for grp in character_grps:\n",
    "    print(grp[0], len(grp[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main characters\n",
    "main_characters = ['C.J. CREGG', 'DONNA MOSS', 'LEO MCGARRY','JOSH LYMAN', 'CHARLIE', 'SAM SEABORN', 'TOBY ZIEGLER', 'BARTLET']\n",
    "filtered_ww = ww_dfs\n",
    "# for char in main_characters:\n",
    "#     filtered_ww = filtered_ww[filtered_ww['character'] == char]\n",
    "\n",
    "filtered_ww = filtered_ww[filtered_ww['character'].isin(main_characters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonwords = []\n",
    "\n",
    "for char in main_characters:\n",
    "    _text = ' '.join(filtered_ww['line'][filtered_ww.character == char])\n",
    "    _words = bag_of_words(nlp(_text))\n",
    "    commonwords = list(set(commonwords+_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = [[nlp(filtered_ww.iloc[ik]['line']), filtered_ww.iloc[ik]['character']] for ik in range(len(filtered_ww))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_df = pd.DataFrame(quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(quotes, common_words, **kwargs):\n",
    "    print(len(quotes))\n",
    "#     df2 = {'line': [sentence[0] for sentence in sentences], 'character': [sentence[1] for sentence in sentences]}\n",
    "    \n",
    "    # sentence stats\n",
    "    sent_stats = ['comma_ct', 'word_ct', 'adv_ct', 'adp_ct', 'propn_ct', 'adj_ct', 'punct_ct'] #'verb_ct', 'noun_ct','det_ct',\n",
    "    if 'sent_stats' in kwargs:\n",
    "        df = pd.DataFrame(columns=list(common_words) + sent_stats)\n",
    "#         df.loc[:, sent_stats] = 0\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=common_words)\n",
    "    \n",
    "    df.loc[:, common_words] = 0\n",
    "    for col in df.columns:\n",
    "        df[col] = np.zeros(len(quotes[0]))\n",
    "    df['line'] = quotes[0] \n",
    "    df['character'] = quotes[1]\n",
    "    \n",
    "    print('made it to the loop')\n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['line']):\n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            try:\n",
    "                df.loc[i, word] += 1\n",
    "            except:\n",
    "                print(word)\n",
    "        \n",
    "        # add sentence features\n",
    "        if 'sent_stats' in kwargs:\n",
    "            commas = 0\n",
    "            for token in sentence:\n",
    "                if token.orth_ == ',':\n",
    "                    commas += 1\n",
    "            df.loc[i, 'comma_ct'] = commas\n",
    "                    \n",
    "            c = Counter([token.pos_ for token in sentence])\n",
    "            for key in c.keys():\n",
    "                if key in pos_d.keys():\n",
    "                    df.loc[i, pos_d[key]] = c[key]\n",
    "            \n",
    "            df.loc[i, 'word_ct'] = len([token for token in sentence if (not token.is_punct)])\n",
    "\n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10474\n",
      "made it to the loop\n",
      "Processing row 0\n",
      "line\n",
      "line\n",
      "line\n",
      "Processing row 1000\n",
      "line\n",
      "line\n",
      "line\n",
      "Processing row 2000\n",
      "line\n",
      "Processing row 3000\n",
      "line\n",
      "Processing row 4000\n",
      "line\n",
      "Processing row 5000\n",
      "Processing row 6000\n",
      "line\n",
      "line\n",
      "line\n",
      "line\n",
      "line\n",
      "line\n",
      "Processing row 7000\n",
      "Processing row 8000\n",
      "Processing row 9000\n",
      "line\n",
      "line\n",
      "line\n",
      "Processing row 10000\n"
     ]
    }
   ],
   "source": [
    "word_counts2 = bow_features(quotes_df, words, kwargs = {'sent_stats' : True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = word_counts2['character']\n",
    "X = word_counts2.iloc[:, ~word_counts2.columns.isin(['character','line'])]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7331, 1751) (7331,)\n",
      "Training set score: 0.4442777247305961\n",
      "\n",
      "Test set score: 0.23639834552974864\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>character</th>\n",
       "      <th>BARTLET</th>\n",
       "      <th>C.J. CREGG</th>\n",
       "      <th>CHARLIE</th>\n",
       "      <th>DONNA MOSS</th>\n",
       "      <th>JOSH LYMAN</th>\n",
       "      <th>LEO MCGARRY</th>\n",
       "      <th>SAM SEABORN</th>\n",
       "      <th>TOBY ZIEGLER</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BARTLET</th>\n",
       "      <td>172</td>\n",
       "      <td>70</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>106</td>\n",
       "      <td>71</td>\n",
       "      <td>63</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C.J. CREGG</th>\n",
       "      <td>27</td>\n",
       "      <td>58</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>49</td>\n",
       "      <td>23</td>\n",
       "      <td>43</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHARLIE</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DONNA MOSS</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JOSH LYMAN</th>\n",
       "      <td>190</td>\n",
       "      <td>212</td>\n",
       "      <td>51</td>\n",
       "      <td>92</td>\n",
       "      <td>330</td>\n",
       "      <td>191</td>\n",
       "      <td>232</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LEO MCGARRY</th>\n",
       "      <td>50</td>\n",
       "      <td>34</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>50</td>\n",
       "      <td>67</td>\n",
       "      <td>40</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAM SEABORN</th>\n",
       "      <td>38</td>\n",
       "      <td>29</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>45</td>\n",
       "      <td>27</td>\n",
       "      <td>59</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOBY ZIEGLER</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>45</td>\n",
       "      <td>27</td>\n",
       "      <td>30</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "character     BARTLET  C.J. CREGG  CHARLIE  DONNA MOSS  JOSH LYMAN  \\\n",
       "row_0                                                                \n",
       "BARTLET           172          70       15          14         106   \n",
       "C.J. CREGG         27          58        7          12          49   \n",
       "CHARLIE             2           7        8           0           7   \n",
       "DONNA MOSS          3           4        1          12           9   \n",
       "JOSH LYMAN        190         212       51          92         330   \n",
       "LEO MCGARRY        50          34       13          19          50   \n",
       "SAM SEABORN        38          29       11           9          45   \n",
       "TOBY ZIEGLER       23          23       10           7          45   \n",
       "\n",
       "character     LEO MCGARRY  SAM SEABORN  TOBY ZIEGLER  \n",
       "row_0                                                 \n",
       "BARTLET                71           63            70  \n",
       "C.J. CREGG             23           43            33  \n",
       "CHARLIE                 4            1             7  \n",
       "DONNA MOSS              4           12             3  \n",
       "JOSH LYMAN            191          232           176  \n",
       "LEO MCGARRY            67           40            38  \n",
       "SAM SEABORN            27           59            21  \n",
       "TOBY ZIEGLER           27           30            37  "
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "\n",
    "y_pred = train.predict(X_test)\n",
    "pd.crosstab(y_pred, y_test, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
