{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "\n",
    "import multiprocessing as mp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script roughly follows the following format\n",
    "\n",
    "CUT TO: Washington Monument - MID DAY\n",
    "\n",
    "CHARACTER1\n",
    "Lorem ipsum \n",
    "\n",
    "Character1 walks across office in some sort of stage direction\n",
    "\n",
    "CHARACTER2 [audio direction]\n",
    "[delivery instruction of some kind] Lorem ipsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# odd-man-out formatting corrections that affect the regularity of the script and thus effectiveness of subsequent slice-and-dice\n",
    "def clean_script(script):\n",
    "    script = script.lstrip('<pre>').split('\\nTHE END')[0]\n",
    "    script = script.replace('McGARRY', 'MCGARRY')\n",
    "    script = script.replace('CUT TO:\\n\\n', 'CUT TO: ')\n",
    "    script_ = re.sub(r'([A-Z])(\\nCUT TO:)', r'\\1\\n\\2',  script)\n",
    "    script = script_.replace('FADE OUT.\\nEND', '')\n",
    "    script = script.replace('FADE OUT.\\n\\nEND ', '')\n",
    "    script = script.rstrip('\\n')\n",
    "     \n",
    "    return script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split script into line elements (line headers and spoken dialogue, stage directions, scene direction, etc.)\n",
    "def make_script_df(script, episode_num):\n",
    "    bits = script.split('\\n\\n')\n",
    "    return pd.DataFrame({'elements':bits, 'episode': [episode_num for ik in range(len(bits))]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label stage directions \n",
    "def parse_stage_dir(script_df):\n",
    "    stage_dir = []\n",
    "    for ik, bit in enumerate(script_df.elements):\n",
    "        if (bit.split('\\n')[0].isupper() == False) and ('[' not in bit.split('\\n')[0]):\n",
    "            stage_dir.append(1)\n",
    "            script_df.loc[ik, 'elements'] = bit.replace('\\n', ' ')\n",
    "        else:\n",
    "            stage_dir.append(0)\n",
    "            \n",
    "    script_df['stage_dir'] = stage_dir\n",
    "    return script_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label scene specifications\n",
    "def parse_scene_set(script_df):\n",
    "    scene_set = []\n",
    "    for elem in script_df.elements:\n",
    "        lines = elem.split('\\n')\n",
    "        if ('-' in lines[0]) and (lines[0].isupper()):\n",
    "            scene_set.append(1)\n",
    "        else:\n",
    "            scene_set.append(0)\n",
    "\n",
    "    script_df['scene_set'] = scene_set\n",
    "    return script_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split out character attribution/line and audio instructions (e.g. VO- voice over) and line delivery instructions (e.g. crossing to pick up the phone)\n",
    "def parse_lines_characters(script_df):\n",
    "    line = []\n",
    "    character = []\n",
    "    deliv_dir = []\n",
    "    audio_dir = []\n",
    "\n",
    "    for ik in range(len(script_df.elements)):\n",
    "        if (script_df.iloc[ik]['scene_set'] == 0) and (script_df.iloc[ik]['stage_dir'] == 0):\n",
    "            tmp_elem  = script_df.iloc[ik]['elements'].split('\\n', 1)\n",
    "            \n",
    "            if len(tmp_elem)>1:\n",
    "                tmp_elem[1] = tmp_elem[1].replace('\\n', ' ')\n",
    "\n",
    "                # character name & audio delivery notes\n",
    "                tmp_audio_dir = tmp_elem[0].split(' [')\n",
    "                if len(tmp_audio_dir)>1:\n",
    "                    audio_dir.append(tmp_audio_dir[1].strip(']'))\n",
    "                    character.append(tmp_audio_dir[0])\n",
    "                else:\n",
    "                    if (tmp_audio_dir[0].isupper() == True):\n",
    "                        audio_dir.append(np.nan)\n",
    "                        character.append(tmp_audio_dir[0])\n",
    "                    else:\n",
    "                        character.append(' ')\n",
    "                        audio_dir.append(np.nan)\n",
    "\n",
    "                # line & acting delivery notes\n",
    "                tmp_deliv_dir = tmp_elem[1].split('] ')\n",
    "                if len(tmp_deliv_dir)>1:\n",
    "                    deliv_dir.append(tmp_deliv_dir[0].strip('['))\n",
    "                    line.append(tmp_deliv_dir[1])\n",
    "                else:\n",
    "                    line.append(tmp_deliv_dir[0])\n",
    "                    deliv_dir.append(np.nan)\n",
    "            else:\n",
    "                line.append(np.nan)\n",
    "                character.append('Drop')\n",
    "                deliv_dir.append(np.nan)\n",
    "                audio_dir.append(np.nan)\n",
    "\n",
    "        else:\n",
    "            line.append(np.nan)\n",
    "            character.append(' ')\n",
    "            deliv_dir.append(np.nan)\n",
    "            audio_dir.append(np.nan)\n",
    "\n",
    "    script_df['line'] = line\n",
    "    script_df['character'] = character\n",
    "    script_df['deliv_dir'] = deliv_dir\n",
    "    script_df['audio_dir'] = audio_dir\n",
    "    \n",
    "    return script_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load scraped scripts\n",
    "ww_df = pd.read_json('WW.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading  59  scripts\n"
     ]
    }
   ],
   "source": [
    "# pull apart scripts\n",
    "script_dfs = []\n",
    "print('loading ', len(ww_df), ' scripts')\n",
    "\n",
    "for ik, text in enumerate(ww_df['text']):\n",
    "    ww_script = text\n",
    "    script_dfs.append(parse_lines_characters(parse_scene_set(parse_stage_dir(make_script_df(clean_script(ww_script),ik+1)))))\n",
    "\n",
    "# concatenate scripts\n",
    "ww_dfs = pd.concat(script_dfs, axis=0, join='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with character names that aren't characters\n",
    "for char in ['DISSOLVE TO', 'FADE OUT.', 'SMASH CUT TO: MAIN TITLES.', 'THE WEST WING', 'ACT ONE', 'ACT TWO', 'END TEASER', 'Drop', ' ']:\n",
    "    ww_dfs = ww_dfs[~ww_dfs['character'].str.contains( char)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize names \n",
    "names_d = {'C.J.': 'C.J. CREGG', 'DONNA':'DONNA MOSS', 'LEO':'LEO MCGARRY', 'JOSH': 'JOSH LYMAN', 'BILLY': 'BILLY KENWORTHY', 'MARY': 'MARY MARSH', 'SAM': 'SAM SEABORN', 'TOBY': 'TOBY ZIEGLER', 'PRESIDENT JED BARTLET': 'BARTLET' }\n",
    "\n",
    "for key, value in names_d.items():\n",
    "    ww_dfs['character'] = ww_dfs['character'].str.replace(value, key)\n",
    "    ww_dfs['character'] = ww_dfs['character'].str.replace(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter data for quotes from main characters  \n",
    "\n",
    "I am limiting the character list to six for the purpose of this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter dataframe so it is limited to primary characters\n",
    "# main characters\n",
    "# main_characters = ['C.J. CREGG', 'AINSLEY', 'DONNA MOSS', 'LEO MCGARRY','JOSH LYMAN', 'CHARLIE', 'SAM SEABORN', 'TOBY ZIEGLER', 'BARTLET']\n",
    "main_characters = ['C.J. CREGG', 'DONNA MOSS', 'LEO MCGARRY','SAM SEABORN', 'TOBY ZIEGLER', 'BARTLET']\n",
    "\n",
    "filtered_ww = ww_dfs\n",
    "filtered_ww = filtered_ww[filtered_ww['character'].isin(main_characters)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter lines to only included longer lines (longer than one word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# add word count so that text can be filtered for longer lines\n",
    "filtered_ww['word_ct'] = [len(s.translate(str.maketrans('', '', string.punctuation)).split(' ')) for s in filtered_ww['line']]\n",
    "\n",
    "filtered_ww_longlines = filtered_ww[filtered_ww.word_ct>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "filtered_ww_longlines['line'] = filtered_ww_longlines['line'].apply(lambda x: text_cleaner(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with different types of word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words approach  \n",
    "\n",
    "My first approach was using a Bag of Words strategy.  As usual, there were multiple avenues for building a feature set...\n",
    "\n",
    "1.) single words or ngrams (n = 2): In hindsight I probably should have used CountVectorizer, but I didn't remember about that tool until I was fairly far in so I stayed the course and wrote by hand.  `make_common_grams` allows the user to pass `two_grams = True` to specify that 2-grams should be added to the word list.  \n",
    "\n",
    "2.) words only or words + sentence statistics: The simplest version uses strictly words as features, but passing `sent_stat = True` to bow_features prompts the function to include information like `comma_ct`, `sent_ct`, and counts of part of speech and repeated phrases.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Utility function to create a list of the 3000 most common words from a block of text.\n",
    "def bag_of_words(text, **kwargs):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    if ('no_stop_words' in kwargs) and (kwargs['no_stop_words'] == True):\n",
    "#         print('bag_of_words: no_stop_words')\n",
    "        allwords = [str(token.lemma_).translate(str.maketrans('', '', string.punctuation))\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                  and not token.is_stop\n",
    "                  and token.is_alpha]\n",
    "    else:\n",
    "#         print('bag_of_words: stop_words')\n",
    "        allwords = [str(token.lemma_).translate(str.maketrans('', '', string.punctuation))\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                  and token.is_alpha]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "# make composite list of ngrams and words\n",
    "def make_common_grams(df, **kwargs):\n",
    "    common_grams = []\n",
    "    all_words = []\n",
    "    all_ngrams = []\n",
    "    if ('no_stop_words' in kwargs) and (kwargs['no_stop_words'] == True):\n",
    "#         print(inspect.stack()[0][3], 'no_stop_words')\n",
    "        nsw = kwargs['no_stop_words']\n",
    "    else:\n",
    "#         print(inspect.stack()[0][3], 'stop_words')\n",
    "        nsw = False\n",
    "\n",
    "    for char in main_characters:\n",
    "        char_text = df['line'][df.character == char]\n",
    "        _text = ' '.join(char_text)\n",
    "        _words = bag_of_words(nlp(_text), no_stop_words = nsw)\n",
    "        \n",
    "        _ngrams = []\n",
    "        if ('two_grams' in kwargs) and (kwargs['two_grams'] == True):\n",
    "#             print(inspect.stack()[0][3], 'two_grams')\n",
    "            # create 2-grams\n",
    "            for line in char_text:\n",
    "                grams = [token.orth_ for token in nlp(line) if (not token.is_punct)]\n",
    "                ngrams = [' '.join([grams[ik].lower(), grams[ik+1].lower()]) for ik in range(len(grams)-1)]\n",
    "                _ngrams = _ngrams+ ngrams\n",
    "        \n",
    "        all_ngrams = all_ngrams + [item[0] for item in Counter(_ngrams).most_common(2000)]\n",
    "        all_words = all_words +_words\n",
    "    \n",
    "    common_grams = list(set(all_words))+ list(set(all_ngrams))\n",
    "    print('length of word list:', len(all_words), 'length if ngram list:', len(all_ngrams), 'length of common word list:', len(common_grams))\n",
    "    return common_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_setups = [\n",
    "    ['commonwords_twograms_nstop', filtered_ww_longlines, {'two_grams': True, 'no_stop_words': True}],\n",
    "    ['commonwords_onegrams_nstop', filtered_ww_longlines, {'two_grams': False, 'no_stop_words': True}],\n",
    "    ['commonwords_twograms_stop', filtered_ww_longlines, {'two_grams': True, 'no_stop_words': False}],\n",
    "    ['commonwords_onegrams_stop', filtered_ww_longlines, {'two_grams': False, 'no_stop_words': False}]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word_unit(word_setup):\n",
    "    return {'label': word_setup[0], 'common_words': make_common_grams(word_setup[1], **word_setup[2]), 'params': word_setup[2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of word list: 12000 length if ngram list: 0 length of common word list: 5192\n",
      "length of word list: 12000 length if ngram list: 0 length of common word list: 4990\n",
      "length of word list: 12000 length if ngram list: 12000 length of common word list: 10689\n",
      "length of word list: 12000 length if ngram list: 12000 length of common word list: 10487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-3:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "pool = mp.Pool(processes=4)\n",
    "word_lists = pool.map(make_word_unit, iter(word_setups))\n",
    "pool.close()\n",
    "\n",
    "word_list_d = {word_lists[ik]['label']: word_lists[ik] for ik in range(len(word_lists))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dataframe of lines and characters where each line is an nlp document\n",
    "def make_quotelist(df):\n",
    "    return  pd.DataFrame([[nlp(df.iloc[ik]['line']), df.iloc[ik]['character'], df.iloc[ik]['word_ct']] for ik in range(len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_df = make_quotelist(filtered_ww_longlines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "\n",
    "# @question: I feel like [from sklearn.feature_extraction.text import CountVectorizer] should be useful here...\n",
    "# POS dict\n",
    "\n",
    "# print(inspect.stack()[0][3], 'no_stop_words') #useful for printing function name with a tag\n",
    "pos_d = {'VERB':'verb_ct', 'NOUN':'noun_ct', 'ADV':'adv_ct', 'ADP':'adp_ct', \n",
    "         'PROPN':'propn_ct', 'ADJ':'adj_ct', 'DET':'det_ct', 'PUNCT':'punct_ct'}\n",
    "\n",
    "def bow_features(quotes, **kwargs):\n",
    "    common_words = kwargs['common_words']\n",
    "    print(len(quotes))\n",
    "    \n",
    "    # sentence stats\n",
    "    sent_stats = ['comma_ct', 'sent_ct', 'word_ct', 'repeats_ct','question_ct', 'comma_freq', 'adv_ct', 'adp_ct', 'propn_ct', 'adj_ct', 'punct_ct','verb_ct', 'noun_ct','det_ct']\n",
    "    if ('sent_stats' in kwargs) and (kwargs['sent_stats'] == True):\n",
    "        cols = list(common_words) + sent_stats\n",
    "    else:\n",
    "        cols = list(common_words)\n",
    "    \n",
    "    df = pd.DataFrame({col: np.zeros(len(quotes[0])) for col in cols})\n",
    "    df['line'] = quotes[0] \n",
    "    df['character'] = quotes[1]\n",
    "    df['word_ct'] = quotes[2]\n",
    "    \n",
    "#     # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['line']):\n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        if ('no_stop_words' in kwargs) and (kwargs['no_stop_words'] == True):\n",
    "            words = [token.lemma_\n",
    "                     for token in sentence\n",
    "                     if (\n",
    "                         not token.is_punct\n",
    "                         and not token.is_stop\n",
    "                         and token.lemma_ in common_words\n",
    "                     )]\n",
    "        else:\n",
    "            words = [token.lemma_\n",
    "                     for token in sentence\n",
    "                     if (\n",
    "                         not token.is_punct\n",
    "                         and token.lemma_ in common_words\n",
    "                     )]\n",
    "        if ('two_grams' in kwargs) and (kwargs['two_grams'] == True):  \n",
    "            grams = [token.orth_ for token in sentence if (not token.is_punct)]\n",
    "            ngrams = [' '.join([grams[ik].lower(), grams[ik+1].lower()]) for ik in range(len(grams)-1)]\n",
    "            words = words+ngrams\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            try:\n",
    "                df.loc[i, word] += 1\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # add sentence features\n",
    "        if ('sent_stats' in kwargs) and (kwargs['sent_stats'] == True):\n",
    "            commas = 0\n",
    "            questions = 0\n",
    "            for token in sentence:\n",
    "                if token.orth_ == ',':\n",
    "                    commas += 1\n",
    "                elif token.orth_ == '?':\n",
    "                    questions +=1\n",
    "            df.loc[i, 'comma_ct'] = commas\n",
    "            df.loc[i, 'question_ct'] = questions\n",
    "                    \n",
    "            # repeated sentece structure\n",
    "            try: ngrams\n",
    "            except NameError:\n",
    "                grams = [token.orth_ for token in sentence if (not token.is_punct)]\n",
    "                ngrams = [' '.join([grams[ik].lower(), grams[ik+1].lower()]) for ik in range(len(grams)-1)]\n",
    "            repeated_phrases = Counter(ngrams)\n",
    "            num_repeats = 0\n",
    "            for phrase in repeated_phrases.keys():\n",
    "                if repeated_phrases[phrase]>1:\n",
    "                    num_repeats +=1\n",
    "            df.loc[i, 'repeats_ct'] = num_repeats\n",
    "                            \n",
    "            # parts of speech count\n",
    "            c = Counter([token.pos_ for token in sentence])\n",
    "            for key in pos_d.keys():\n",
    "                if key in c.keys():\n",
    "                    df.loc[i, pos_d[key]] = c[key]\n",
    "                    \n",
    "            if df.loc[i, 'word_ct'] >0:\n",
    "                df.loc[i, 'comma_freq'] = commas/df.loc[i, 'word_ct']\n",
    "            else:\n",
    "                df.loc[i, 'comma_freq'] = 0\n",
    "            df.loc[i, 'sent_ct'] = len([sent for sent in sentence.sents])\n",
    "\n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 5000 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep features without sentence stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "def make_test_train(quotes_df, **kwargs):\n",
    "    word_counts2 = bow_features(quotes_df, **kwargs)\n",
    "        \n",
    "    print('done')\n",
    "    word_counts2 = word_counts2.dropna()\n",
    "\n",
    "    Y = word_counts2['character']\n",
    "    X = word_counts2.iloc[:, ~word_counts2.columns.isin(['character','line'])]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                        Y,\n",
    "                                                        test_size=0.3,\n",
    "                                                        random_state=0)\n",
    "    \n",
    "    return {'X_train': X_train, 'X_test': X_test, 'y_train':y_train, 'y_test':y_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_unit(model_setup):\n",
    "    return {'label':model_setup[0], 'data': make_test_train(model_setup[1], **model_setup[2]), 'params': model_setup[2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_setups = [\n",
    "    ['noss_twograms_stop', quotes_df, {'common_words': word_list_d['commonwords_twograms_stop']['common_words'], 'sent_stats': False, 'no_stop_words': False, 'two_grams': True}], \n",
    "    ['ss_twograms_stop', quotes_df, {'common_words': word_list_d['commonwords_twograms_stop']['common_words'], 'sent_stats': True, 'no_stop_words': False, 'two_grams': True}], \n",
    "    ['ss_twograms_nstop', quotes_df, {'common_words': word_list_d['commonwords_twograms_nstop']['common_words'], 'sent_stats': True, 'no_stop_words': True, 'two_grams': True}],\n",
    "    ['noss_twograms_nstop', quotes_df, {'common_words': word_list_d['commonwords_twograms_nstop']['common_words'], 'sent_stats': False, 'no_stop_words': True, 'two_grams': True}],\n",
    "    ['ss_onegrams_stop', quotes_df, {'common_words': word_list_d['commonwords_onegrams_stop']['common_words'], 'sent_stats': True, 'no_stop_words': False, 'two_grams': False}],\n",
    "    ['noss_onegrams_stop', quotes_df, {'common_words': word_list_d['commonwords_onegrams_stop']['common_words'], 'sent_stats': False, 'no_stop_words': False, 'two_grams': False}],\n",
    "    ['ss_onegrams_nstop', quotes_df, {'common_words': word_list_d['commonwords_onegrams_nstop']['common_words'], 'sent_stats': True, 'no_stop_words': True, 'two_grams': False}],\n",
    "    ['noss_onegrams_nstop', quotes_df, {'common_words': word_list_d['commonwords_onegrams_nstop']['common_words'], 'sent_stats': False, 'no_stop_words': True, 'two_grams': False}]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18207\n",
      "Processing row 0\n",
      "18207\n",
      "Processing row 0\n",
      "18207\n",
      "Processing row 0\n",
      "Processing row 5000\n",
      "Processing row 5000\n",
      "Processing row 5000\n",
      "Processing row 10000\n",
      "Processing row 10000\n",
      "Processing row 10000\n",
      "Processing row 15000\n",
      "Processing row 15000\n",
      "Processing row 15000\n",
      "done\n",
      "18207\n",
      "Processing row 0\n",
      "done\n",
      "done\n",
      "18207\n",
      "Processing row 0\n",
      "18207\n",
      "Processing row 0\n",
      "Processing row 5000\n",
      "Processing row 5000\n",
      "Processing row 10000\n",
      "Processing row 15000\n",
      "Processing row 10000\n",
      "done\n",
      "18207\n",
      "Processing row 0\n",
      "Processing row 15000\n",
      "Processing row 5000\n",
      "done\n",
      "18207\n",
      "Processing row 0\n",
      "Processing row 10000\n",
      "Processing row 5000\n",
      "Processing row 5000\n",
      "Processing row 10000\n",
      "Processing row 15000\n",
      "Processing row 15000\n",
      "done\n",
      "done\n",
      "Processing row 10000\n",
      "Processing row 15000\n",
      "done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Pool.close of <multiprocessing.pool.Pool object at 0x11e6ce8d0>>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-7:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.stdout = open('/dev/stdout', 'w')\n",
    "\n",
    "pool = mp.Pool(processes=3)\n",
    "results = pool.map(make_data_unit, iter(model_setups))\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def train_test_log_reg(d):\n",
    "    data = d['data']\n",
    "    gs_params = {'C': 1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
    "    lr = LogisticRegression(multi_class = 'auto', solver = 'lbfgs')\n",
    "    lr.fit(data['X_train'], data['y_train'])\n",
    "    d['model'] = lr\n",
    "    d['train_score'] = lr.score(data['X_train'], data['y_train'])\n",
    "    d['test_score'] = lr.score(data['X_test'], data['y_test'])\n",
    "    print(d['label'], 'Training set score:',d['train_score'], 'Test set score:', d['test_score'] )\n",
    "\n",
    "    y_pred = lr.predict(data['X_test'])\n",
    "#     pd.crosstab(y_pred, data['y_test'], dropna=False)\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noss_twograms_stop Training set score: 0.47073132454488387 Test set score: 0.271279516749039\n",
      "ss_twograms_stop Training set score: 0.4331450094161959 Test set score: 0.26578802855573863\n",
      "ss_twograms_nstop Training set score: 0.40952605147520404 Test set score: 0.26285923485264506\n",
      "ss_onegrams_stop Training set score: 0.3312146892655367 Test set score: 0.24693391909207396\n",
      "noss_twograms_nstop Training set score: 0.4971751412429379 Test set score: 0.2736591616328025\n",
      "noss_onegrams_stop Training set score: 0.3597771500313873 Test set score: 0.26578802855573863\n",
      "ss_onegrams_nstop Training set score: 0.3401600753295669 Test set score: 0.25846604429800474\n",
      "noss_onegrams_nstop Training set score: 0.3828468298807282 Test set score: 0.2535237049240344\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "pool = mp.Pool(processes=3)\n",
    "results = pool.map(train_test_log_reg, iter(results))\n",
    "pool.close()\n",
    "\n",
    "results_d = {results[ik]['label']:results[ik] for ik in range(len(results))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The case to beat is:\n",
    "    `ss_onegrams_stop Training set score: 0.3312146892655367 Test set score: 0.24693391909207396`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [[label, results_d[label]['test_score']] for label in results_d.keys()]\n",
    "label_ind = np.array([scores[ik][1] for ik in range(len(scores))]).argmax()\n",
    "label = scores[label_ind][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test alternate models\n",
    "data = results_d[label]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noss_twograms_nstop Training set score: 0.21720025109855617 Test set score: 0.233205198608823\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>character</th>\n",
       "      <th>BARTLET</th>\n",
       "      <th>C.J. CREGG</th>\n",
       "      <th>DONNA MOSS</th>\n",
       "      <th>LEO MCGARRY</th>\n",
       "      <th>SAM SEABORN</th>\n",
       "      <th>TOBY ZIEGLER</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BARTLET</th>\n",
       "      <td>1274</td>\n",
       "      <td>952</td>\n",
       "      <td>534</td>\n",
       "      <td>933</td>\n",
       "      <td>952</td>\n",
       "      <td>818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "character  BARTLET  C.J. CREGG  DONNA MOSS  LEO MCGARRY  SAM SEABORN  \\\n",
       "row_0                                                                  \n",
       "BARTLET       1274         952         534          933          952   \n",
       "\n",
       "character  TOBY ZIEGLER  \n",
       "row_0                    \n",
       "BARTLET             818  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(data['X_train'], data['y_train'])\n",
    "print(label, 'Training set score:',svc.score(data['X_train'], data['y_train']), 'Test set score:', svc.score(data['X_test'], data['y_test']) )\n",
    "\n",
    "y_pred = svc.predict(data['X_test'])\n",
    "pd.crosstab(y_pred, data['y_test'], dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noss_twograms_nstop Training set score: 0.38292529817953547 Test set score: 0.2756727073036793\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>character</th>\n",
       "      <th>BARTLET</th>\n",
       "      <th>C.J. CREGG</th>\n",
       "      <th>DONNA MOSS</th>\n",
       "      <th>LEO MCGARRY</th>\n",
       "      <th>SAM SEABORN</th>\n",
       "      <th>TOBY ZIEGLER</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BARTLET</th>\n",
       "      <td>1130</td>\n",
       "      <td>735</td>\n",
       "      <td>430</td>\n",
       "      <td>713</td>\n",
       "      <td>757</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C.J. CREGG</th>\n",
       "      <td>50</td>\n",
       "      <td>124</td>\n",
       "      <td>23</td>\n",
       "      <td>64</td>\n",
       "      <td>48</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DONNA MOSS</th>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>41</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LEO MCGARRY</th>\n",
       "      <td>35</td>\n",
       "      <td>29</td>\n",
       "      <td>11</td>\n",
       "      <td>73</td>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAM SEABORN</th>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>28</td>\n",
       "      <td>68</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOBY ZIEGLER</th>\n",
       "      <td>25</td>\n",
       "      <td>29</td>\n",
       "      <td>15</td>\n",
       "      <td>38</td>\n",
       "      <td>48</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "character     BARTLET  C.J. CREGG  DONNA MOSS  LEO MCGARRY  SAM SEABORN  \\\n",
       "row_0                                                                     \n",
       "BARTLET          1130         735         430          713          757   \n",
       "C.J. CREGG         50         124          23           64           48   \n",
       "DONNA MOSS         14          12          41           17           14   \n",
       "LEO MCGARRY        35          29          11           73           17   \n",
       "SAM SEABORN        20          23          14           28           68   \n",
       "TOBY ZIEGLER       25          29          15           38           48   \n",
       "\n",
       "character     TOBY ZIEGLER  \n",
       "row_0                       \n",
       "BARTLET                635  \n",
       "C.J. CREGG              50  \n",
       "DONNA MOSS              14  \n",
       "LEO MCGARRY             25  \n",
       "SAM SEABORN             24  \n",
       "TOBY ZIEGLER            70  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(data['X_train'], data['y_train'])\n",
    "print(label, 'Training set score:',gbc.score(data['X_train'], data['y_train']), 'Test set score:', gbc.score(data['X_test'], data['y_test']) )\n",
    "\n",
    "y_pred = gbc.predict(data['X_test'])\n",
    "pd.crosstab(y_pred, data['y_test'], dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed: 11.8min remaining:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed: 11.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 25, 'class_weight': None, 'max_iter': 100, 'multi_class': 'auto', 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.2791506498261029\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "            'penalty':['l2'],\n",
    "            'C':[1,25],\n",
    "            'solver': ['lbfgs'],#,'newton-cg',  'liblinear', 'sag'],\n",
    "            'class_weight': ['balanced', None],\n",
    "            'max_iter': [100],\n",
    "            'multi_class':['auto']\n",
    "        }\n",
    "lr = LogisticRegression()\n",
    "GS = GridSearchCV(lr, parameters,cv=5,verbose=5, n_jobs = -1)\n",
    "GS.fit(data['X_train'], data['y_train'])\n",
    "\n",
    "new_params = GS.best_params_\n",
    "print(new_params)\n",
    "print(GS.score(data['X_test'], data['y_test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  15 | elapsed: 11.5min remaining:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed: 15.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 25, 'class_weight': None, 'max_iter': 100, 'multi_class': 'auto', 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.29452681676734394\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "            'penalty':['l2'],\n",
    "            'C':[25],\n",
    "            'solver': ['lbfgs','liblinear', 'sag'],\n",
    "            'class_weight': [None],\n",
    "            'max_iter': [100],\n",
    "            'multi_class':['auto']\n",
    "        }\n",
    "lr = LogisticRegression()\n",
    "GS2 = GridSearchCV(lr, parameters,cv=5,verbose=5, n_jobs = -1)\n",
    "GS2.fit(data['X_train'], data['y_train'])\n",
    "\n",
    "new_params2 = GS2.best_params_\n",
    "print(new_params2)\n",
    "print(GS2.score(data['X_test'], data['y_test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end I did manage a 5% bump from the lowest score to the highest score by picking the feature set and model/model parameters exhaustively.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF  \n",
    "Now let's consider a TF-IDF approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 6307\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(filtered_ww_longlines['line'], filtered_ww_longlines['character'], test_size=0.3, random_state=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.6, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=3, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             ngram_range = (1,2),\n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "ww_tfidf=vectorizer.fit_transform(filtered_ww_longlines['line'])\n",
    "print(\"Number of features: %d\" % ww_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(ww_tfidf,filtered_ww_longlines['character'],  test_size=0.3, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 58.67301306043237\n",
      "Component 25:\n",
      "line\n",
      "No, sir.                               0.575166\n",
      "I will, sir.                           0.575166\n",
      "No, sir.                               0.575166\n",
      "No, sir.                               0.575166\n",
      "Sir, I...                              0.575166\n",
      "This is take five, sir.                0.575166\n",
      "What, were you inconvenienced, Sir?    0.575166\n",
      "I have my concerns, sir.               0.575166\n",
      "No, sir.                               0.575166\n",
      "No, sir.                               0.575166\n",
      "Name: 25, dtype: float64\n",
      "Component 26:\n",
      "line\n",
      "Please, tell me it's not...                                 0.890899\n",
      "Tell her where you are.                                     0.890899\n",
      "No, but I have to tell them.                                0.890899\n",
      "Tell me about yourself.                                     0.890899\n",
      "Tell him it's done.                                         0.890899\n",
      "Tell me when it's done.                                     0.890899\n",
      "Tell me more, Obi-Wan.                                      0.890899\n",
      "Tell me again.                                              0.890899\n",
      "Tell them that.                                             0.890899\n",
      "You can't tell her. You have to tell her something else.    0.890899\n",
      "Name: 26, dtype: float64\n",
      "Component 27:\n",
      "line\n",
      "Would you talk to Leo?                  0.624827\n",
      "Talk to me.                             0.548308\n",
      "Talk to us.                             0.548308\n",
      "You talk to C.J.?                       0.548308\n",
      "Why can't she talk?                     0.548308\n",
      "Can I talk to you?                      0.548308\n",
      "Talk to C.J.                            0.548308\n",
      "Talk to me about what?                  0.548308\n",
      "Then what can we do but talk to him?    0.548308\n",
      "Can I talk to him?                      0.548308\n",
      "Name: 27, dtype: float64\n",
      "Component 28:\n",
      "line\n",
      "That's what we're here to talk about.    0.712436\n",
      "Talk to us.                              0.712436\n",
      "Why can't she talk?                      0.712436\n",
      "Talk to C.J.                             0.712436\n",
      "Can I talk to him?                       0.712436\n",
      "Then what can we do but talk to him?     0.712436\n",
      "Talk to me about what?                   0.712436\n",
      "Talk to me.                              0.712436\n",
      "You talk to C.J.?                        0.712436\n",
      "Can I talk to you?                       0.712436\n",
      "Name: 28, dtype: float64\n",
      "Component 29:\n",
      "line\n",
      "I'm fine with it.                                   0.786315\n",
      "I'm fine.                                           0.786315\n",
      "That's fine.                                        0.786315\n",
      "No, it's fine.                                      0.786315\n",
      "You seem fine.                                      0.786315\n",
      "Everything was fine except the part where we go.    0.786315\n",
      "I'm fine.                                           0.786315\n",
      "He's fine!                                          0.786315\n",
      "Fine, but she's on your payroll.                    0.786315\n",
      "So, everything's fine.                              0.786315\n",
      "Name: 29, dtype: float64\n",
      "Component 30:\n",
      "line\n",
      "You sure?                                                                    0.916349\n",
      "Are you sure it's not nephrology, immunology, cardiology, or dermatology?    0.916349\n",
      "I'm sure of it.                                                              0.916349\n",
      "You're sure?                                                                 0.916349\n",
      "You sure?                                                                    0.916349\n",
      "Sure, about what?                                                            0.916349\n",
      "Sure you will.                                                               0.916349\n",
      "You sure?                                                                    0.916349\n",
      "You sure?                                                                    0.916349\n",
      "Sure. About what?                                                            0.916349\n",
      "Name: 30, dtype: float64\n",
      "Component 31:\n",
      "line\n",
      "Oh, dear God.        0.740743\n",
      "Oh, Merciful god.    0.740743\n",
      "Oh my God...         0.710145\n",
      "Oh, God...           0.710145\n",
      "Oh, my God!          0.710145\n",
      "Oh, God.             0.710145\n",
      "Oh, my God.          0.710145\n",
      "Oh God.              0.710145\n",
      "Oh, my God.          0.710145\n",
      "Oh, God.             0.710145\n",
      "Name: 31, dtype: float64\n",
      "Component 32:\n",
      "line\n",
      "What are you talking about?            0.583778\n",
      "What are you talking about?            0.583778\n",
      "They're talking about someone else.    0.583778\n",
      "What're you talking about?             0.583778\n",
      "Talking to myself.                     0.583778\n",
      "What are you talking about?            0.583778\n",
      "What are you talking about?            0.583778\n",
      "What are you talking about?            0.583778\n",
      "I was talking to you.                  0.583778\n",
      "What are you talking about?            0.583778\n",
      "Name: 32, dtype: float64\n",
      "Component 33:\n",
      "line\n",
      "Hey.              0.652117\n",
      "Hey, C.J..        0.652117\n",
      "Hey, Spanky.      0.652117\n",
      "Hey, C.J.         0.652117\n",
      "Hey, doc!         0.652117\n",
      "Hey Toscanini.    0.652117\n",
      "Hey, Mal.         0.652117\n",
      "Hey, Nimrod.      0.652117\n",
      "Hey, Gracie...    0.652117\n",
      "Hey, C.J.         0.652117\n",
      "Name: 33, dtype: float64\n",
      "Component 34:\n",
      "line\n",
      "It's no problem.                 0.875112\n",
      "What's the problem?              0.875112\n",
      "No problem. Was it you?          0.875112\n",
      "Well, I have a problem, so...    0.875112\n",
      "So, what's your problem?         0.875112\n",
      "What's your problem?             0.875112\n",
      "What's the problem?              0.875112\n",
      "What's the problem?              0.875112\n",
      "How is this my problem?          0.875112\n",
      "Here's my problem.               0.875112\n",
      "Name: 34, dtype: float64\n",
      "Component 35:\n",
      "line\n",
      "Find out what happened.           0.814745\n",
      "What happened to...?              0.814745\n",
      "What happened?                    0.814745\n",
      "What happened?                    0.814745\n",
      "What happened?                    0.814745\n",
      "What happened?                    0.814745\n",
      "What happened?                    0.814745\n",
      "And then it happened.             0.814745\n",
      "What happened with Richardson?    0.814745\n",
      "What happened?                    0.814745\n",
      "Name: 35, dtype: float64\n",
      "Component 36:\n",
      "line\n",
      "I'm not saying anything.                    0.736407\n",
      "This is what I'm saying.                    0.736407\n",
      "You're saying that if she's a lesbian...    0.736407\n",
      "Are you saying...                           0.736407\n",
      "What are you saying that to me for?         0.736407\n",
      "I'm saying...                               0.736407\n",
      "I'm saying...                               0.736407\n",
      "That's what you're saying to me?            0.736407\n",
      "Are you saying it was you?                  0.736407\n",
      "I'm saying...                               0.736407\n",
      "Name: 36, dtype: float64\n",
      "Component 37:\n",
      "line\n",
      "Get me Leo!                         0.727240\n",
      "It's Leo.                           0.727240\n",
      "Where's Leo?                        0.727240\n",
      "Leo...                              0.727240\n",
      "Where's Leo?                        0.727240\n",
      "Give it to Leo.                     0.727240\n",
      "Call me Leo.                        0.727240\n",
      "Leo, who made these assignments?    0.727240\n",
      "(warning tone) Leo...               0.722438\n",
      "It's a vicious circle, Leo.         0.722405\n",
      "Name: 37, dtype: float64\n",
      "Component 38:\n",
      "line\n",
      "I'm saying...                          0.51983\n",
      "I'm saying how'd it...                 0.51983\n",
      "What are you saying?                   0.51983\n",
      "How many are they saying?              0.51983\n",
      "What are you saying?                   0.51983\n",
      "Are you saying it was you?             0.51983\n",
      "This is what I'm saying.               0.51983\n",
      "I'm not saying anything.               0.51983\n",
      "That's what you're saying to me?       0.51983\n",
      "What are you saying that to me for?    0.51983\n",
      "Name: 38, dtype: float64\n",
      "Component 39:\n",
      "line\n",
      "There's no way you have time for this.         0.602263\n",
      "The greatest musician of his time.             0.479996\n",
      "It's time to go.                               0.478811\n",
      "Is it time?                                    0.478811\n",
      "What time was that?                            0.478811\n",
      "What other time?                               0.478811\n",
      "He will have traveled back in time to what?    0.478811\n",
      "Some of us have had more time than others!     0.478811\n",
      "It's time.                                     0.478811\n",
      "How much time do you have?                     0.478811\n",
      "Name: 39, dtype: float64\n",
      "Component 40:\n",
      "line\n",
      "Look, I...               0.644663\n",
      "Take a look.             0.644663\n",
      "Look ...                 0.644663\n",
      "Look, if...              0.644663\n",
      "Look, if...              0.644663\n",
      "Look at them.            0.644663\n",
      "You can look it over.    0.644663\n",
      "Look, if I may...        0.644663\n",
      "Take a look.             0.644663\n",
      "Look, I...               0.644663\n",
      "Name: 40, dtype: float64\n",
      "Component 41:\n",
      "line\n",
      "I'm not kidding.      0.953744\n",
      "No kidding.           0.953744\n",
      "Are you kidding?      0.953744\n",
      "I'm kidding.          0.953744\n",
      "No kidding.           0.953744\n",
      "No kidding.           0.953744\n",
      "You're kidding?       0.953744\n",
      "Are you kidding?      0.953744\n",
      "You're kidding me!    0.953744\n",
      "Are you kidding?      0.953744\n",
      "Name: 41, dtype: float64\n",
      "Component 42:\n",
      "line\n",
      "Is that the same thing?          0.737127\n",
      "There's... such a thing as...    0.737127\n",
      "Give me the thing.               0.737127\n",
      "Is my thing in there?            0.737127\n",
      "It's a northeastern thing?       0.737127\n",
      "This is a serious thing now.     0.737127\n",
      "There's no such thing.           0.737127\n",
      "The thing is...                  0.737127\n",
      "Here's the thing.                0.737127\n",
      "Here's the thing.                0.737127\n",
      "Name: 42, dtype: float64\n",
      "Component 43:\n",
      "line\n",
      "Some of us have had more time than others!     0.513279\n",
      "How much time do you have?                     0.513279\n",
      "Is it time?                                    0.513279\n",
      "It's time.                                     0.513279\n",
      "Take your time.                                0.513279\n",
      "He will have traveled back in time to what?    0.513279\n",
      "What time was that?                            0.513279\n",
      "It's time to go.                               0.513279\n",
      "It's time.                                     0.513279\n",
      "What other time?                               0.513279\n",
      "Name: 43, dtype: float64\n",
      "Component 44:\n",
      "line\n",
      "Here's what we're gonna do.                                0.616111\n",
      "You're gonna have to.                                      0.616111\n",
      "We're gonna do something?                                  0.616111\n",
      "We're gonna have to do this again.                         0.616111\n",
      "I'm gonna get C.J.                                         0.616111\n",
      "It's gonna be relaxed. It's gonna be relaxed.              0.616111\n",
      "I'm gonna stretch my legs.                                 0.612837\n",
      "I'm gonna rip his arms off...                              0.611530\n",
      "I'm gonna crush him.                                       0.607650\n",
      "You're gonna sacrifice the Bishop for the Queen's rook.    0.602254\n",
      "Name: 44, dtype: float64\n",
      "Component 45:\n",
      "line\n",
      "Why are people footing the bill for this anyway?           0.556799\n",
      "Why are people here?                                       0.556799\n",
      "You're with the ratings people?                            0.556799\n",
      "You're not, you're not, you're not one of those people!    0.556799\n",
      "...people get on each other's nerves.                      0.556799\n",
      "I had our people...                                        0.556799\n",
      "Which one of our people?                                   0.556799\n",
      "These people have done this before.                        0.556799\n",
      "The people.                                                0.556799\n",
      "There are people out there.                                0.556799\n",
      "Name: 45, dtype: float64\n",
      "Component 46:\n",
      "line\n",
      "There's no way you'd find it yourself.    0.745761\n",
      "It's on its way.                          0.745761\n",
      "No way. I can't.                          0.745761\n",
      "I'm that way.                             0.745761\n",
      "In what way?                              0.745761\n",
      "Are they on their way?                    0.745761\n",
      "OPEC will find a way to punish us.        0.745761\n",
      "There's no way we can do it.              0.745761\n",
      "He's on his way down.                     0.745761\n",
      "No way.                                   0.745761\n",
      "Name: 46, dtype: float64\n",
      "Component 47:\n",
      "line\n",
      "Thanks.                          0.593063\n",
      "Thanks, Rodney.                  0.593063\n",
      "Thanks. C'mon in.                0.593063\n",
      "No. Thanks.                      0.593063\n",
      "Thanks.                          0.593063\n",
      "This is C.J. Thanks.             0.593063\n",
      "'I will do it again.' Thanks.    0.593063\n",
      "No. Thanks.                      0.593063\n",
      "Thanks.                          0.593063\n",
      "No. Thanks.                      0.593063\n",
      "Name: 47, dtype: float64\n",
      "Component 48:\n",
      "line\n",
      "Sam? Are you guarding my office?              0.570153\n",
      "Toby, are you in here sticking up for Sam?    0.546048\n",
      "Sam, what the hell...!                        0.477805\n",
      "Thank you. [to Toby and Sam                   0.458499\n",
      "They do, Sam!                                 0.445386\n",
      "Sam's still on.                               0.445386\n",
      "Sam can do this.                              0.445386\n",
      "Where's Sam?                                  0.445386\n",
      "Sam, it was...                                0.445386\n",
      "Sam, that's enough.                           0.445386\n",
      "Name: 48, dtype: float64\n",
      "Component 49:\n",
      "line\n",
      "When you said...                          0.704150\n",
      "something to be said for...               0.704150\n",
      "I said I'd do it, Buckminster!            0.704150\n",
      "He said, \"What's next?\"                   0.704150\n",
      "You said...                               0.704150\n",
      "They said you were in here.               0.704150\n",
      "Who said it was?                          0.704150\n",
      "That's what I said.                       0.704150\n",
      "You said one of the wheels was wobbly.    0.702898\n",
      "When I said I was assigned to you?        0.701667\n",
      "Name: 49, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 1379 to 130.\n",
    "svd= TruncatedSVD(500)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "X_test_lsa = lsa.fit_transform(X_test_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(25, 50,1):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12744,) (12744,)\n",
      "Training set score: 0.33976773383553044\n",
      "\n",
      "Test set score: 0.19897492220391727\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>character</th>\n",
       "      <th>BARTLET</th>\n",
       "      <th>C.J. CREGG</th>\n",
       "      <th>DONNA MOSS</th>\n",
       "      <th>LEO MCGARRY</th>\n",
       "      <th>SAM SEABORN</th>\n",
       "      <th>TOBY ZIEGLER</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BARTLET</th>\n",
       "      <td>444</td>\n",
       "      <td>304</td>\n",
       "      <td>165</td>\n",
       "      <td>306</td>\n",
       "      <td>323</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C.J. CREGG</th>\n",
       "      <td>189</td>\n",
       "      <td>175</td>\n",
       "      <td>86</td>\n",
       "      <td>172</td>\n",
       "      <td>142</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DONNA MOSS</th>\n",
       "      <td>47</td>\n",
       "      <td>45</td>\n",
       "      <td>25</td>\n",
       "      <td>22</td>\n",
       "      <td>30</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LEO MCGARRY</th>\n",
       "      <td>156</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>137</td>\n",
       "      <td>125</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAM SEABORN</th>\n",
       "      <td>214</td>\n",
       "      <td>151</td>\n",
       "      <td>93</td>\n",
       "      <td>146</td>\n",
       "      <td>166</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOBY ZIEGLER</th>\n",
       "      <td>224</td>\n",
       "      <td>149</td>\n",
       "      <td>101</td>\n",
       "      <td>150</td>\n",
       "      <td>166</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "character     BARTLET  C.J. CREGG  DONNA MOSS  LEO MCGARRY  SAM SEABORN  \\\n",
       "row_0                                                                     \n",
       "BARTLET           444         304         165          306          323   \n",
       "C.J. CREGG        189         175          86          172          142   \n",
       "DONNA MOSS         47          45          25           22           30   \n",
       "LEO MCGARRY       156         128          64          137          125   \n",
       "SAM SEABORN       214         151          93          146          166   \n",
       "TOBY ZIEGLER      224         149         101          150          166   \n",
       "\n",
       "character     TOBY ZIEGLER  \n",
       "row_0                       \n",
       "BARTLET                259  \n",
       "C.J. CREGG             155  \n",
       "DONNA MOSS              39  \n",
       "LEO MCGARRY             99  \n",
       "SAM SEABORN            126  \n",
       "TOBY ZIEGLER           140  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train_lsa, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train_lsa, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test_lsa, y_test))\n",
    "\n",
    "y_pred = train.predict(X_test_lsa)\n",
    "pd.crosstab(y_pred, y_test, dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, the LSA features only did slightly better than naive guessing.  Indeed, unlike with the Bag of Words approach, in each instance the model guessed a given line belonged to Bartlet more often than it did the actual character.  \n",
    "\n",
    "I suspect LSA is most useful when author voices are quite distinct, or the corpus represents a number of very distinct topics.  Neither this case, nor the exercises using Jane Austen prose make an argument for its usefulness in a one author scenario.  (Small sample size, but I'm starting to suspect that differentiation is in delivery, not in gross syntax or grammer.  That said, with more time, I suspect I could develop a richer feature set that would capture more of differences between character voices.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an exercise, I implemented some published code from a blog post about Gensim.  It yielded very little, but I leave it here for reference.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame({'line': X_train, 'character': y_train})\n",
    "test = pd.DataFrame({'line': X_test, 'character': y_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagged = train.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['line']), tags=[r.character]), axis=1)\n",
    "test_tagged = test.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['line']), tags=[r.character]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14851/14851 [00:00<00:00, 584368.66it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14851/14851 [00:00<00:00, 802711.49it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 2253522.26it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1979395.87it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1611799.64it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 2160433.15it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1975942.42it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 2022455.56it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 986437.92it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 2368876.54it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1905230.58it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 2134448.44it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 2356598.39it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1764478.18it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 2089273.79it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 2124909.90it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 2041880.57it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 2173474.60it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1615813.46it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1892380.87it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1331913.71it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1525883.32it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 2369417.20it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 2194919.08it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 2023243.86it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 2343124.01it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1668665.35it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1813908.23it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1865070.03it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 856120.41it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1900522.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.3 s, sys: 4.87 s, total: 26.2 s\n",
      "Wall time: 18.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.17609173735469683\n",
      "Testing F1 score: 0.151498149323639\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14851/14851 [00:00<00:00, 1134022.88it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14851/14851 [00:00<00:00, 1626615.36it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1644089.23it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1763828.65it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1816394.27it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1673147.51it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1487619.62it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 2244023.66it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 2127668.01it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1695878.27it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1596350.81it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 2248235.35it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1471419.67it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1372804.00it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 2215608.19it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 2234925.50it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1777925.18it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1669783.63it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1180890.44it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1088788.83it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 827120.38it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1499039.99it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1557357.02it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1628741.99it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1641879.08it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1996397.83it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1358374.23it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1860779.95it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1029291.09it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 1503599.31it/s]\n",
      "100%|██████████| 14851/14851 [00:00<00:00, 2028779.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.7 s, sys: 12.3 s, total: 46 s\n",
      "Wall time: 33.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dmm.alpha -= 0.002\n",
    "    model_dmm.min_alpha = model_dmm.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.20750863964813068\n",
      "Testing F1 score: 0.20835794709831132\n"
     ]
    }
   ],
   "source": [
    "y_train, X_train = vec_for_learning(model_dmm, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dmm, test_tagged)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = get_vectors(new_model, train_tagged)\n",
    "y_test, X_test = get_vectors(new_model, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.20562362551052465\n",
      "Testing F1 score: 0.20666847253973306\n"
     ]
    }
   ],
   "source": [
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all shouting', 'at once', 'at the', 'be full', 'briefing tonight', 'folks listen', 'full briefing', 'get you', 'going to', 'in moment', 'is going', 'listen up', 'll be', 'moment or', 'my name', 'name at', 'once there', 'or two', 'president is', 'say something', 'shouting my', 'something that', 'sure to', 'that sure', 'the president', 'the white', 'there ll', 'to get', 'to say', 'tonight at', 'two the', 'up in', 'white house', 'you all']\n",
      "Folks, listen up. In a moment or two, the President is going to say something that's sure to get you all shouting my name at once. There'll be a full briefing tonight at the White House.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range = (2,2))\n",
    "vectorizer.fit_transform([filtered_ww_longlines['line'].iloc[3]])\n",
    "print(vectorizer.get_feature_names())\n",
    "print(filtered_ww_longlines['line'].iloc[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
