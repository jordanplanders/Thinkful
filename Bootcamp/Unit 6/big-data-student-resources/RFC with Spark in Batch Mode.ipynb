{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Random Forest Classifier with Spark in Batch Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you'll learn the basics of working with Spark in batch mode to build a random forest classifier. Before digging in, make sure to read the background context in the related reading in the curriculum!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Import dependencies\n",
    "\n",
    "First off, we need to import the tools we'll need from PySpark. The imports below allow us to connect to the Spark server, load our data, clean it, and prepare, execute, and evaluate a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.sql.functions import isnan, when, count, col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set our constants\n",
    "\n",
    "Next, we create a set of constants that we can refer to throughout the notebook. These are values that the rest of our code needs to run, but that we might need to change at some point (for instance, if the location of our data changes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"/home/ds/notebooks/datasets/UCI_HAR/allData.csv\"\n",
    "CSV_ACTIVITY_LABEL_PATH = \"/home/ds/notebooks/datasets/UCI_HAR/activity_labels.csv\"\n",
    "APP_NAME = \"UCI HAR Random Forest Example\"\n",
    "SPARK_URL = \"local[*]\"\n",
    "RANDOM_SEED = 141107\n",
    "TRAINING_DATA_RATIO = 0.8\n",
    "RF_NUM_TREES = 10\n",
    "RF_MAX_DEPTH = 4\n",
    "RF_NUM_BINS = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the server and load data\n",
    "\n",
    "Now we're ready to connect to the Spark server. We do that (relying on the constants set above) and then load our labels (loaded into `activity_labels`) and activity data (loaded into `df`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(APP_NAME).master(SPARK_URL).getOrCreate()\n",
    "activity_labels = spark.read.options(inferschema = \"true\").csv(CSV_ACTIVITY_LABEL_PATH)\n",
    "df = spark.read.options(inferschema = \"true\").csv(CSV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the data\n",
    "\n",
    "If our data has been properly cleaned and prepared, it will meet the following criteria, which we'll verify in just a moment:\n",
    "\n",
    "* The dataframe shape should be 10,299 rows by 562 columns\n",
    "* All feature columns should be doubles. Note than one of the columns is for our label and it will not be double.\n",
    "* There should be no nulls. This point is crucial because Spark will fail to build our vector variables for our classifier if there are any null values.\n",
    "\n",
    "Let's confirm these points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape is 10299 rows by 562 columns.\n"
     ]
    }
   ],
   "source": [
    "# Confirm the dataframe shape is 10,299 rows by 562 columns\n",
    "print(f\"Dataset shape is {df.count():d} rows by {len(df.columns):d} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561 columns out of 562 total are type double.\n"
     ]
    }
   ],
   "source": [
    "# Confirm that all feature columns are doubles via a list comprehension\n",
    "# We're expecting 561 of 562 here, accounting for the labels column\n",
    "double_cols = [col[0] for col in df.dtypes if col[1] == 'double']\n",
    "print(f\"{len(double_cols):d} columns out of {len(df.columns):d} total are type double.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 null values in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Confirm there are no null values. We use the dataframe select method to build a \n",
    "# list that is then converted to a Python dict. This way it's easy to sum up the nulls.\n",
    "null_counts = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) \n",
    "                         for c in df.columns]).toPandas().to_dict(orient='records')\n",
    "\n",
    "print(f\"There are {sum(null_counts[0].values()):d} null values in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up and run our classifier in Spark\n",
    "\n",
    "After confirming our data is clean, we're ready to reshape the data and run the random forest model.\n",
    "\n",
    "In Spark, we manipulate the data to work in a Spark pipeline, define each of the steps in the pipeline, chain them together, and finally run the pipeline.\n",
    "\n",
    "Apache Spark classifiers expect 2 columns of input:\n",
    "\n",
    "1. __labels__: an indexed set of numeric variables that represent the classification from the set of features we provide.\n",
    "2. __features__: an indexed, vector variable that contains all of the feature values in each row. \n",
    "\n",
    "In order to do this, we need to create these 2 columns from our dataset - the data is there, but not yet in a format we can use in the classifier.\n",
    "\n",
    "To create the indexed labels column, we'll create a column called `indexedLabel` using the `StringIndexer` method. We use the column `_c0` as the source for our label index since that contains our labels. The column contains only one value per index.\n",
    "    \n",
    "To create the indexed features column, we'll need to do two things. First, we'll create the vector of features using the `VectorAssembler` method. To create this vector, we'll need to use all 561 numeric columns from our data frame. The vector assembler will create a new column called `features`, and each row of this column will contain a 561-element vector that is built from the 561 features in the dataset.\n",
    "\n",
    "Finally, we'll complete the data preparation by creating an indexed vector from the `features` column. We'll call this vector `indexedFeatures`.\n",
    "    \n",
    "Since the classifier expects indexed labels and an indexed vector column of data, we'll use the `indexedLabel` and `indexedFeatures` as inputs to our random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate our feature vector.\n",
    "# Note that we're doing the work on the `df` object - we don't create new dataframes, \n",
    "# just add columns to the one we already are using.\n",
    "\n",
    "# the transform method creates the column.\n",
    "\n",
    "df = VectorAssembler(inputCols=double_cols, outputCol=\"features\").transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that the features are there. It's easy to do this in Apache Spark using the `select` and `show` methods on the dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|_c0|            features|\n",
      "+---+--------------------+\n",
      "|  5|[0.289,-0.0203,-0...|\n",
      "|  5|[0.278,-0.0164,-0...|\n",
      "|  5|[0.28,-0.0195,-0....|\n",
      "|  5|[0.279,-0.0262,-0...|\n",
      "|  5|[0.277,-0.0166,-0...|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"_c0\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to build the indexers, split our data for training and testing, define our model, and finally chain everything together into a pipeline.\n",
    "\n",
    "__It's important to note that when we execute this cell, we're not actually running our model. At this point, we're only defining its parameters__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the training indexers / split data / classifier\n",
    "# first we'll generate a labelIndexer\n",
    "labelIndexer = StringIndexer(inputCol=\"_c0\", outputCol=\"indexedLabel\").fit(df)\n",
    "\n",
    "# now generate the indexed feature vector\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(df)\n",
    "    \n",
    "# Split the data into training and validation sets (30% held out for testing)\n",
    "(trainingData, testData) = df.randomSplit([TRAINING_DATA_RATIO, 1 - TRAINING_DATA_RATIO])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=RF_NUM_TREES)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell runs the pipeline, delivering a trained model at the end of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now easy to test our model and make predictions simply by using the model's `transform` method on the `testData` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "\n",
    "Now we can use the MulticlassClassificationEvaluator to test the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.0816038\n",
      "Accuracy = 0.918396\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Test Error = {(1.0 - accuracy):g}\")\n",
    "print(f\"Accuracy = {accuracy:g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "We've seen how to prepare data and build a classifier in Spark. You might want to play around with this notebook and learn more about how Spark works. Here are some ideas:\n",
    "\n",
    "- Look at the set of labels, and see if there are any features that would make sense to combine. Spark allows you to map values into a new column.\n",
    "- Identify the most important features among the 561 source features (using PCA or something similar), then reduce the feature set and see if the model performs better.\n",
    "- Modify the settings of the random forest to see if the performance improves.\n",
    "- Use Spark's tools to find other techniques to evaluate the performance of your model. See if you can figure out how to generate an ROC plot, find the AUC value, or plot a confusion matrix."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
