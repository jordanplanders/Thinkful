{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding Web Servers in Streaming Network Data With Spark\n",
    "-----------------------------------------------------\n",
    "\n",
    "It's time to run our streaming experiment. In this notebook, we'll do a few things:\n",
    "1. Import all the modules we need to run this in Spark.\n",
    "2. Set up our constants and our Spark session.\n",
    "3. First, test our hypothesis about web servers on the directory of JSON files using batch processing.\n",
    "4. Finally, stream the JSON files through Spark, run our query, and compare our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------\n",
    "\n",
    "First, import everything we need for our project.\n",
    "Note that we're importing a number of data types - this is needed to define our streaming JSON schema. This is probably the most important part of the streaming setup. It's critical to get it right up front.\n",
    "\n",
    "We handle our streaming through the `StreamingContext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import desc, col, window\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "import json\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define your input path - yours may differ from the one below. Remember, though - it's important that this be the path **inside** your container; not the local one on your laptop.\n",
    "\n",
    "We also get the number of files in the directory, then get an \"offset\" value we'll use later on to signal when our query is completed.\n",
    "\n",
    "Now, start your `SparkSession`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50 files in our inputPath, which gives an offset of 49.\n"
     ]
    }
   ],
   "source": [
    "inputPath = \"/home/ds/notebooks/datasets/lanl/\"\n",
    "\n",
    "numFiles = len(os.listdir(inputPath))\n",
    "numFileOffset = numFiles - 1\n",
    "\n",
    "print(f\"There are {numFiles} files in our inputPath, which gives an offset of {numFileOffset}.\")\n",
    "\n",
    "APP_NAME = \"Web Server Hypothesis Test\"\n",
    "SPARK_URL = \"local[*]\"\n",
    "\n",
    "spark = SparkSession.builder.appName(APP_NAME).master(SPARK_URL).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our schema for the streaming data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowSchema = StructType([\n",
    "    StructField('time', TimestampType(), True),\n",
    "    StructField('duration', LongType(), True),\n",
    "    StructField('srcdevice', StringType(), True),\n",
    "    StructField('dstdevice', StringType(), True),\n",
    "    StructField('protocol', LongType(), True),\n",
    "    StructField('srcport', StringType(), True),\n",
    "    StructField('dstport', StringType(), True),\n",
    "    StructField('srcpackets', LongType(), True),\n",
    "    StructField('dstpackets', LongType(), True),\n",
    "    StructField('srcbytes', LongType(), True),\n",
    "    StructField('dstbytes', LongType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've set up the session and all supporting variables, it's time to run some queries.\n",
    "\n",
    "First, we'll create a static dataframe from all files in the directory. Spark makes this easy since we can pass it either a single filename, or a directory that contains our files, and it handles them the same in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static DataFrame representing data in the JSON files\n",
    "staticInputDF = spark.read.json(inputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the schema. We don't need to apply our schema to this static data frame, so we don't do so at this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dstbytes: double (nullable = true)\n",
      " |-- dstdevice: string (nullable = true)\n",
      " |-- dstpackets: double (nullable = true)\n",
      " |-- dstport: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- protocol: double (nullable = true)\n",
      " |-- srcbytes: double (nullable = true)\n",
      " |-- srcdevice: string (nullable = true)\n",
      " |-- srcpackets: double (nullable = true)\n",
      " |-- srcport: string (nullable = true)\n",
      " |-- time: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "staticInputDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to build our query. It's a pretty simple SQL-like query.\n",
    "\n",
    "Recall that each row in the dataset represents a single conversation between two devices. A web server should be queried on ports 80 and 443 disproportionately more than devices that are not web servers.\n",
    "\n",
    "We select the `dstdevice` column, but limit only to the rows where the `dstport` is either `80` or `443`. Then we group by the `dstdevice` and get the count of each `dstdevice`.\n",
    "\n",
    "Next, we sort by the count descending and show the top 20 web servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|          dstdevice|count|\n",
      "+-------------------+-----+\n",
      "|EnterpriseAppServer|14495|\n",
      "|         Comp576843|14153|\n",
      "|         Comp186884|12681|\n",
      "|         Comp501516| 5859|\n",
      "|         Comp393033| 3795|\n",
      "|         Comp916004| 3332|\n",
      "|         Comp498128| 2831|\n",
      "|         Comp573929| 2555|\n",
      "|         Comp611862| 2404|\n",
      "|         Comp370444| 2385|\n",
      "|         Comp097048| 1991|\n",
      "|         Comp847595| 1886|\n",
      "|         Comp574103| 1629|\n",
      "|         Comp657655| 1590|\n",
      "|         Comp309567| 1576|\n",
      "|         Comp216677| 1528|\n",
      "|         Comp509586| 1516|\n",
      "|         Comp336938| 1501|\n",
      "|         Comp146745| 1451|\n",
      "|         Comp457448| 1180|\n",
      "+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "staticInputDF.select('dstdevice') \\\n",
    "    .where(col('dstport').isin([80, 443])) \\\n",
    "    .groupby('dstdevice') \\\n",
    "    .count() \\\n",
    "    .sort(desc('count')) \\\n",
    "    .show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our static baseline in place, let's try to replicate it in the streaming context.\n",
    "\n",
    "The good news here is that Spark treats a Streaming dataframe just like a static / batch dataframe. So the code you'll see below should be very familiar.\n",
    "\n",
    "First, we set up a streaming input data frame. This gets the rows from our JSON, one file at a time.\n",
    "\n",
    "We simply tell Spark to read a stream limited to one file at a time, apply the defined schema, and use the JSON interpreter on the directory specified in `inputPath`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingInputDF = (\n",
    "  spark\n",
    "    .readStream                       \n",
    "    .schema(flowSchema)               # Set the schema of the JSON data\n",
    "    .option(\"maxFilesPerTrigger\", 1)  # Treat a sequence of files as a stream by picking one file at a time\n",
    "    .json(inputPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming handles things _slightly_ different - we need to create a streaming counts dataframe; we can't query the input dataframe directly.\n",
    "\n",
    "This is where we define our query - note that it looks almost identical to the static dataframe we saw earlier - and we confirm to Spark that yes, our counts dataframe is indeed a streaming dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamingCountsDF = streamingInputDF \\\n",
    "    .select('dstdevice') \\\n",
    "    .where(col('dstport').isin([80, 443])) \\\n",
    "    .groupBy(streamingInputDF.dstdevice) \\\n",
    "    .count() \\\n",
    "    .sort(desc('count'))\n",
    "\n",
    "# Is this DF actually a streaming DF?\n",
    "streamingCountsDF.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to start our streaming engine. We do this by creating an object named `query` that writes the stream into an in-memory table called `counts`. This is the table we'll monitor during streaming to see the progress of our count of web servers.\n",
    "\n",
    "We also set a shuffles partition to a small value here.\n",
    "\n",
    "The streaming process only begins when we execute the `.start()` method on the `streamingCountsDF` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n",
    "\n",
    "query = (\n",
    "  streamingCountsDF\n",
    "    .writeStream\n",
    "    .format(\"memory\")       \n",
    "    .queryName(\"counts\")     # counts = name of the in-memory table\n",
    "    .outputMode(\"complete\")  # complete = all the counts should be in the table\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we monitor the query as it proceeds. We let the query run a little by sleeping for four seconds; then we fall into a loop that updates each second until all files have been processed and the query has stopped. Unfortunately there isn't an easy way to see that the query has stopped, so we use our `numFileOffset` value to match with the `logOffset` in the `recentProgress` structure. Once they're equal, we terminate our loop, knowing we've run through all of the files in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|          dstdevice|count|\n",
      "+-------------------+-----+\n",
      "|EnterpriseAppServer|12170|\n",
      "|         Comp576843|12161|\n",
      "|         Comp186884|11271|\n",
      "|         Comp501516| 4692|\n",
      "|         Comp393033| 3114|\n",
      "|         Comp916004| 2862|\n",
      "|         Comp573929| 2300|\n",
      "|         Comp611862| 2085|\n",
      "|         Comp370444| 2070|\n",
      "|         Comp498128| 1984|\n",
      "|         Comp097048| 1782|\n",
      "|         Comp847595| 1647|\n",
      "|         Comp657655| 1590|\n",
      "|         Comp309567| 1421|\n",
      "|         Comp574103| 1401|\n",
      "|         Comp216677| 1373|\n",
      "|         Comp336938| 1344|\n",
      "|         Comp509586| 1335|\n",
      "|         Comp146745| 1224|\n",
      "|         Comp801677| 1029|\n",
      "+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------------+-----+\n",
      "|          dstdevice|count|\n",
      "+-------------------+-----+\n",
      "|EnterpriseAppServer|13202|\n",
      "|         Comp576843|13103|\n",
      "|         Comp186884|11933|\n",
      "|         Comp501516| 5335|\n",
      "|         Comp393033| 3466|\n",
      "|         Comp916004| 3082|\n",
      "|         Comp498128| 2742|\n",
      "|         Comp573929| 2325|\n",
      "|         Comp370444| 2240|\n",
      "|         Comp611862| 2232|\n",
      "|         Comp097048| 1882|\n",
      "|         Comp847595| 1753|\n",
      "|         Comp657655| 1590|\n",
      "|         Comp574103| 1524|\n",
      "|         Comp309567| 1498|\n",
      "|         Comp216677| 1443|\n",
      "|         Comp509586| 1427|\n",
      "|         Comp336938| 1424|\n",
      "|         Comp146745| 1331|\n",
      "|         Comp801677| 1097|\n",
      "+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------------+-----+\n",
      "|          dstdevice|count|\n",
      "+-------------------+-----+\n",
      "|EnterpriseAppServer|13975|\n",
      "|         Comp576843|13744|\n",
      "|         Comp186884|12373|\n",
      "|         Comp501516| 5740|\n",
      "|         Comp393033| 3713|\n",
      "|         Comp916004| 3206|\n",
      "|         Comp498128| 2743|\n",
      "|         Comp370444| 2339|\n",
      "|         Comp573929| 2325|\n",
      "|         Comp611862| 2320|\n",
      "|         Comp097048| 1946|\n",
      "|         Comp847595| 1833|\n",
      "|         Comp574103| 1590|\n",
      "|         Comp657655| 1590|\n",
      "|         Comp309567| 1551|\n",
      "|         Comp216677| 1490|\n",
      "|         Comp509586| 1476|\n",
      "|         Comp336938| 1463|\n",
      "|         Comp146745| 1401|\n",
      "|         Comp457448| 1162|\n",
      "+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let the query run for a bit to insure there is data in the recent progress structure.\n",
    "time.sleep(4)\n",
    "\n",
    "# Monitor the progress of the query. The last table should be identical to the static query.\n",
    "while True:\n",
    "    spark.sql(\"select * from counts\").show(20)\n",
    "    time.sleep(1)\n",
    "    if query.recentProgress[-1]['sources'][0]['endOffset']['logOffset'] == numFileOffset:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's it!\n",
    "\n",
    "If you compare the last table in the streaming example to the static dataframe query, you'll see that the two are identical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
