{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "So far we've covered decision trees growing with an entropy criterion. In doing so, however, we glossed over how that would actually work. In this assignment we'll give more detail into how an algorithm to do that would practically function.\n",
    "\n",
    "Here we'll cover one popular algorithm for building a decision tree using entropy and information gain: **ID3**.\n",
    "\n",
    "## The ID3 Algorithm\n",
    "\n",
    "ID3 stands for _Iterative Dichotomizer 3_, and is one of the simplest ways to create a decision tree. It can be generalized into more robust scenarios, but the implementation is based on the framework we'll go over here. Essentially ID3 goes through every feature to find the most valuable attribute and then splits based on it. It moves further and further down the tree until it either has a pure class or has met a terminating condition. We'll cover the details below.\n",
    "\n",
    "Before you can start working with ID3, however, there are some requirements for the data in this most basic form. Firstly, outcomes have to be binary. The simplest version of ID3 is a binary classifier. The attributes we're using to build the tree also have to be categorical. Attributes can have many categories but they must be known and countable.\n",
    "\n",
    "If those two criteria are met then you can build a basic ID3 classifying algorithm.\n",
    "\n",
    "The other thing we'll need for this is our definition of entropy. Recall from the previous assignment that we're going to use Shannon Entropy $H$, defined as:\n",
    "\n",
    "$$ H = -\\sum_{i=1}^n P(x_i)log_2 P(x_i) $$\n",
    "\n",
    "For simplicity of calculation, we're actually going to do a slight transform on this definition. Recall from a (quite possibly long ago) algebra class that you can bring exponentials out to the front of a logarithm. In this case we'll raise the probability to -1, changing the formula to:\n",
    "\n",
    "$$ H = \\sum_{i=1}^n P(x_i)log_2 \\frac{1}{P(x_i)} $$\n",
    "\n",
    "This removes the negative sign up front and will make it easier for us to implement this formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Entropy\n",
    "\n",
    "Since this algorithm is based on entropy let's go over a quick example of how to calculate it.\n",
    "\n",
    "Let's say we have 20 students, and we're trying to classify them as male and female. The only attribute we have is whether their height is tall, medium, or short. Of the 20 students, 12 are boys with and 8 are girls. Of the 12 boys, 4 are tall, 6 are medium and 2 are short. Of the 8 girls, 1 is tall, 2 are medium, and 5 are short.\n",
    "\n",
    "What is the entropy, both before any rule is applied and then after applying a rule for being tall?\n",
    "\n",
    "The initial entropy is just the formula plugged in over both the possible classes of interest:\n",
    "\n",
    "$$ H = P(male)*log_2 \\frac{1}{P(male)} + P(female)*log_2 \\frac{1}{P(female)}  $$\n",
    "\n",
    "\n",
    "$$ = \\frac{12}{20}*log_2 \\frac{20}{12} + \\frac{8}{20}*log_2 \\frac{20}{8} = .971 $$\n",
    "\n",
    "What if we apply the rule _height = short_? We need to calculate the weighted average of the two entropies, one for the short students and one for the non-short students.\n",
    "\n",
    "$$ H(short) = P(male)*log_2 \\frac{1}{P(male)} + P(female)*log_2 \\frac{1}{P(female)}  $$\n",
    "\n",
    "$$ = \\frac{2}{7}*log_2 \\frac{7}{2} + \\frac{5}{7}*log_2 \\frac{7}{5} = .863 $$\n",
    "\n",
    "$$ H(not\\_short) = P(male)*log_2 \\frac{1}{P(male)} + P(female)*log_2 \\frac{1}{P(female)}  $$\n",
    "\n",
    "$$ = \\frac{10}{13}*log_2 \\frac{13}{10} + \\frac{3}{13}*log_2 \\frac{13}{3} = .779 $$\n",
    "\n",
    "Note that all the probabilities here are conditional on the criteria we're assuming (short or not short). Now the weighted average of the two is just:\n",
    "\n",
    "$$ P(short) * H(short) + P(not\\_short) * H(not\\_short) = \\frac{7}{20} * .863 + \\frac{13}{20} * .779 = .809 $$\n",
    "\n",
    "So our entropy from this question would go from .971 to .809. That's an improvement. Use the space below to calculate the entropy of the other criteria, for we asked whether the students were tall or medium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from math import log2\n",
    "import numpy as np\n",
    "\n",
    "# Put your calculations below\n",
    "\n",
    "\n",
    "def calc_H(M, F, ik):\n",
    "    inds = [0,1,2]\n",
    "    inds.remove(ik)\n",
    "    ip = inds[0]\n",
    "    ij = inds[1]\n",
    "    \n",
    "    H_x = M[ik]/(M[ik]+F[ik]) * log2((M[ik]+F[ik])/M[ik]) + F[ik]/(M[ik]+F[ik]) * log2((M[ik]+F[ik])/F[ik])\n",
    "\n",
    "    sum_M = sum([M[im] for im in inds])\n",
    "    sum_F = sum([F[im] for im in inds])\n",
    "    \n",
    "    H_non_x = sum_M/(sum_M+sum_F)*log2((sum_M+sum_F)/sum_M)+(sum_F)/(sum_M+sum_F)*log2((sum_M+sum_F)/sum_F)\n",
    "\n",
    "\n",
    "    perc_x = (M[ik]+F[ik])/(sum(M)+sum(F))    \n",
    "    H_x_tot = perc_x*H_x + (1-perc_x)*H_non_x\n",
    "\n",
    "    return H_x_tot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short: 0.808669593238176\n",
      "medium: 0.9245112497836532\n",
      "tall: 0.9280757477080679\n"
     ]
    }
   ],
   "source": [
    "\n",
    "M = [2,6,4]\n",
    "F = [5,2,1]\n",
    "\n",
    "print('short:', calc_H(M, F, 0))\n",
    "print('medium:', calc_H(M, F, 1))\n",
    "print('tall:', calc_H(M, F, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Day','Outlook','Temp','Humidity','Wind','Decision']\n",
    "data = [[1,'Sunny','Hot','High','Weak','No'],\n",
    "[2,'Sunny','Hot','High','Strong','No'],\n",
    "[3,'Overcast','Hot','High','Weak','Yes'],\n",
    "[4,'Rain','Mild','High','Weak','Yes'],\n",
    "[5,'Rain','Cool','Normal','Weak','Yes'],\n",
    "[6,'Rain','Cool','Normal','Strong','No'],\n",
    "[7,'Overcast','Cool','Normal','Strong','Yes'],\n",
    "[8,'Sunny','Mild','High','Weak','No'],\n",
    "[9,'Sunny','Cool','Normal','Weak','Yes'],\n",
    "[10,'Rain','Mild','Normal','Weak','Yes'],\n",
    "[11,'Sunny','Mild','Normal','Strong','Yes'],\n",
    "[12,'Overcast','Mild','High','Strong','Yes'],\n",
    "[13,'Overcast','Hot','Normal','Weak','Yes'],\n",
    "[14,'Rain','Mild','High','Strong','No']\n",
    "]\n",
    "\n",
    "\n",
    "N = [row[1:] for row in data if row[-1]== 'No']\n",
    "Y = [row[1:] for row in data if row[-1]== 'Yes']\n",
    "\n",
    "data2 = []\n",
    "target = []\n",
    "for row in data:\n",
    "    target.append(row[-1])\n",
    "    data2.append(row[1:-1])\n",
    "\n",
    "cols = []\n",
    "for ind in range(len(data2[0])):\n",
    "    cols.append([row[ind] for row in data2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode = []\n",
    "for im in range(len(data2[0])):\n",
    "    decode.append({elem: ind for ind, elem in enumerate(set([data2[ik][im] for ik in range(len(data2))]))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "ik = 0\n",
    "H_meta = []\n",
    "H_vals = []\n",
    "for ik in range(len(data2[0])):\n",
    "    N_feat = [row[ik] for row in N]\n",
    "    Y_feat = [row[ik] for row in Y]\n",
    "\n",
    "    N_d = {elem:0 for elem in list(decode[ik].keys())}\n",
    "    Y_d = {elem:0 for elem in list(decode[ik].keys())}\n",
    "\n",
    "\n",
    "    for elem in N_feat:\n",
    "        N_d[elem]+=1\n",
    "\n",
    "    for elem in Y_feat:\n",
    "        Y_d[elem]+=1\n",
    "\n",
    "    N_ = []\n",
    "    Y_ = []\n",
    "    labels = []\n",
    "    for key in N_d.keys():\n",
    "        N_.append(N_d[key])\n",
    "        Y_.append(Y_d[key])\n",
    "        labels.append((ik, key))\n",
    "    \n",
    "    for label_ind, label in enumerate(labels):\n",
    "        try:\n",
    "    #         print(label,':', calc_H(N_, Y_, label_ind))\n",
    "            H_ = calc_H(N_, Y_, label_ind)\n",
    "        except:\n",
    "            H_ = 1\n",
    "        H_meta.append([label[1], label[0], label_ind, H_])\n",
    "        H_vals.append(H_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sunny', 0, 0, 0.8380423950607804]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_meta[np.argmin(H_vals)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9280757477080679 0.9245112497836532\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE SOLUTION\n",
    "\n",
    "# Tall.\n",
    "H_tall = 4 / 5 * log2(5 / 4) + 1 / 5 * log2(5 / 1)\n",
    "H_not_tall = 8 / 15 * log2(15 / 8) + 7 / 15 * log2(15 / 7)\n",
    "\n",
    "entropy_tall = 5 / 20 * H_tall + 15 / 20 * H_not_tall\n",
    "\n",
    "\n",
    "# Medium.\n",
    "H_medium = 6/8 * log2(8/6) + 2/8 * log2(8/2)\n",
    "H_not_medium = 6/12 * log2(12/6) + 6/12 * log2(12/6)\n",
    "\n",
    "entropy_medium = 8/20 * (H_medium) + 12/20 * (H_not_medium)\n",
    "\n",
    "print(entropy_tall, entropy_medium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "You should have found entropies of .925 for medium and .928 for tall. Both of those entropies are higher. Now, we've previously said we want to prioritize questions with the most information gain. Which one of these would that be?\n",
    "\n",
    "It would be asking if an individual is short. You should also note that for all possible questions, we're still comparing with the same initial entropy value. So one way of seeing which question has the most information gain is to find the one with the lowest entropy. This should make sense when we think of entropy as uncertainty. The less uncertainty after a question, the more information we gained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocoding the Algorithm\n",
    "\n",
    "**Pseudocode** is the processes of writing the steps and logic you would implement in code, but in normal language rather than commands a programming language could execute. It can be a useful way to chart out how you want to build an algorithm and is a common topic for technical interviews. Here we'll use pseudocode to explain the ID3 algorithm.\n",
    "\n",
    "Here is reasonable pseudocode for ID3, which we'll then follow up with an explanation of the steps. The outcome for this variable will be A or B. An attribute is denoted a<sub>i</sub>. A value of that attribute is v<sub>i</sub>.\n",
    "\n",
    "\n",
    "<pre>\n",
    "Algorithm(Observations, Outcome, Attributes)\n",
    "    Create a root node.\n",
    "    If all observations are 'A', label root node 'A' and return.\n",
    "    If all observations are 'B', label root node 'B' and return.\n",
    "    If no attributes return the root note labeled with the most common Outcome.\n",
    "    Otherwise, start:\n",
    "        For each value v<sub>i</sub> of each attribute a<sub>i</sub>, calculate the entropy.\n",
    "        The attribute a<sub>i</sub> and value v<sub>i</sub> with the lowest entropy is the best rule.\n",
    "        The attribute for this node is then a<sub>i</sub>\n",
    "            Split the tree to below based on the rule a<sub>i</sub> = v<sub>i</sub>\n",
    "            Observations<sub>v<sub>i</sub></sub> is the subset of observations with value v<sub>i</sub>\n",
    "            If Observations<sub>v<sub>i</sub></sub> is empty cap with node labeled with most common Outcome\n",
    "            Else at the new node start a subtree (Observations<sub>v<sub>i</sub></sub>, Target Outcome, Attributes - {a<sub>i</sub>}) and repeat the algorithm\n",
    "</pre>\n",
    "\n",
    "Now let's walk through this pseudocode algorithm in plain English piece by piece.\n",
    "\n",
    "First you create a root node. Simple enough, you have to start with something.\n",
    "\n",
    "The next two lines say that if you're already exclusively one class, just label with that class and you're done. Similarly the following line says that if you don't have any attributes left you're also done, labeling with the most common outcome.\n",
    "\n",
    "Then we get into the _real_ algorithm. First you have to find the best attribute by calculating entropies for all possible values. The attribute value pair with the lowest entropy is then the best attribute and the attribute you give to the node.\n",
    "\n",
    "You use that rule to split the node, creating subsets of observations. There are then two new nodes, each with a subset of the observations corresponding to the rule. If obsevations are null then label with the dominant outcome.\n",
    "\n",
    "Otherwise at each new node start the algorithm again.\n",
    "\n",
    "This is how a decision tree would actually function. Understanding this should give you some insight into how this algorithm works and reveals several attributes of the algorithm. Firstly, the solution is not necessarily optimal. The tree can get stuck in local optima, just like with the gradient descent algorithm. It also has no way to work backwards if it finds itself in an informationless space. You can add criteria to make it stop before the tree has grown to run out of attributes or all leaf nodes are single classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRILL:\n",
    "\n",
    "Look over the code for [this real](https://github.com/NinjaSteph/DecisionTree/blob/master/sna2111_DecisionTree/DecisionTree.py) ID3 Algorithm in python. Note how well the author breaks up functionality into individual, reusable, well-named helper functions. See if you can match our pseudocode steps to the code in this example."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "68px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
